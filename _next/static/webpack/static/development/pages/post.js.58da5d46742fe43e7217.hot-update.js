webpackHotUpdate("static/development/pages/post.js",{

/***/ "../posts-built/2019/pandoc-url2cite.md.json":
/*!***************************************************!*\
  !*** ../posts-built/2019/pandoc-url2cite.md.json ***!
  \***************************************************/
/*! exports provided: filename, frontmatter, preview, content_ast, default */
/***/ (function(module) {

module.exports = JSON.parse("{\"filename\":\"2019/pandoc-url2cite.md\",\"frontmatter\":{\"link-citations\":true,\"references\":[{\"note\":\"original-date: 2019-08-19T22:24:50Z\",\"URL\":\"https://github.com/phiresky/pandoc-url2cite\",\"title-short\":\"Effortlessly and transparently add correctly styled citations to your markdown paper given only a URL\",\"author\":[{\"literal\":\"phiresky\"}],\"id\":\"https://github.com/phiresky/pandoc-url2cite\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"issued\":{\"date-parts\":[[\"2019\",\"12\"]]},\"title\":\"Effortlessly and transparently add correctly styled citations to your markdown paper given only a URL: Phiresky/pandoc-url2cite\",\"type\":\"no-type\"},{\"URL\":\"https://citationstyles.org/\",\"container-title\":\"Citation Style Language\",\"author\":[{\"family\":\"Name\",\"given\":\"Your\"}],\"id\":\"https://citationstyles.org/\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"abstract\":\"CitationStyles.org, home of the Citation Style Language (CSL), a popular open XML-based language to describe the formatting of citations and bibliographies.\",\"title\":\"Citation Style Language\",\"type\":\"no-type\"},{\"URL\":\"https://www.mendeley.com/?interaction_required=true\",\"container-title\":\"www.mendeley.com\",\"id\":\"https://www.mendeley.com/\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"abstract\":\"Mendeley is a free reference manager and an academic social network. Manage your research, showcase your work, connect and collaborate with over five million researchers worldwide.\",\"title\":\"Mendeley - Reference Management Software & Researcher Network\",\"type\":\"no-type\"},{\"URL\":\"https://www.zotero.org/\",\"container-title\":\"www.zotero.org\",\"id\":\"https://www.zotero.org/\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"abstract\":\"Zotero is a free, easy-to-use tool to help you collect, organize, cite, and share research.\",\"title\":\"Zotero Your personal research assistant\",\"type\":\"no-type\"},{\"URL\":\"https://phiresky.github.io/blog/2019/pandoc-url2cite/\",\"container-title\":\"phiresky.github.io\",\"id\":\"https://phiresky.github.io/blog/2019/pandoc-url2cite/\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"abstract\":\"pandoc-url2cite[1] allows you to instantly and transparently cite most papers directly given only a single URL. You simply add a URL of a publication, and it will replace that with a real citation in whatever CSL[2] style you want. This means you can avoid dealing with Mendeley[3] or Zotero[4] and\",\"title\":\"Automatic citation extraction from URLs (draft) - phiresky’s blog\",\"type\":\"no-type\"},{\"URL\":\"https://opensource.com/article/18/9/pandoc-research-paper\",\"container-title\":\"Opensource.com\",\"author\":[{\"family\":\"Kiko\",\"given\":\"Fernandez-Reyes\"}],\"id\":\"https://opensource.com/article/18/9/pandoc-research-paper\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"abstract\":\"Learn how to manage section references, figures, tables, and more in Markdown.\",\"title\":\"How to use Pandoc to produce a research paper\",\"type\":\"no-type\"},{\"URL\":\"https://pandoc.org/MANUAL.html#citations\",\"container-title\":\"pandoc.org\",\"id\":\"https://pandoc.org/MANUAL.html#citations\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"title\":\"Pandoc - Pandoc User’s Guide\",\"type\":\"no-type\"},{\"URL\":\"https://pandoc.org/MANUAL.html#extension-yaml_metadata_block\",\"container-title\":\"pandoc.org\",\"id\":\"https://pandoc.org/MANUAL.html#extension-yaml_metadata_block\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"title\":\"Pandoc - Pandoc User’s Guide\",\"type\":\"no-type\"},{\"note\":\"original-date: 2013-09-20T08:26:14Z\",\"URL\":\"https://github.com/retorquere/zotero-better-bibtex\",\"author\":[{\"family\":\"Heyns\",\"given\":\"Emiliano\"}],\"id\":\"https://github.com/retorquere/zotero-better-bibtex\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"issued\":{\"date-parts\":[[\"2019\",\"12\"]]},\"title\":\"Make Zotero effective for us LaTeX holdouts. Contribute to retorquere/zotero-better-bibtex development by creating an account on GitHub\",\"type\":\"no-type\"},{\"URL\":\"https://www.zotero.org/support/dev/translators\",\"title-short\":\"Dev\",\"container-title\":\"www.zotero.org\",\"id\":\"https://www.zotero.org/support/dev/translators\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"abstract\":\"Zotero is a free, easy-to-use tool to help you collect, organize, cite, and share research.\",\"title\":\"Dev:Translators [Zotero Documentation]\",\"type\":\"no-type\"},{\"note\":\"original-date: 2018-06-11T11:28:53Z\",\"URL\":\"https://github.com/zotero/translation-server\",\"id\":\"https://github.com/zotero/translation-server\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"issued\":{\"date-parts\":[[\"2019\",\"12\"]]},\"title\":\"A Node.Js-based server to run Zotero translators. Contribute to zotero/translation-server development by creating an account on GitHub\",\"type\":\"no-type\",\"publisher\":\"zotero\"},{\"URL\":\"https://www.mediawiki.org/wiki/Citoid/API\",\"container-title\":\"www.mediawiki.org\",\"id\":\"https://www.mediawiki.org/wiki/Citoid/API\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"title\":\"Citoid/API - MediaWiki\",\"type\":\"no-type\"},{\"URL\":\"https://github.com/zotero/translation-server/issues/70\",\"container-title\":\"GitHub\",\"id\":\"https://github.com/zotero/translation-server/issues/70\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"abstract\":\"Related to #38, but a few translators are able to function based on the URL, even when it&#39;s a PDF page. We should try to support those cases, before either trying PDF recognition (from #38) or ...\",\"title\":\"Try translating PDF URLs based on URL · Issue #70 · zotero/translation-server\",\"type\":\"no-type\"},{\"URL\":\"https://www.overleaf.com/learn/latex/Bibtex_bibliography_styles\",\"container-title\":\"www.overleaf.com\",\"id\":\"https://www.overleaf.com/learn/latex/Bibtex_bibliography_styles\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"abstract\":\"An online LaTeX editor that’s easy to use. No installation, real-time collaboration, version control, hundreds of LaTeX templates, and more.\",\"title\":\"Bibtex bibliography styles\",\"type\":\"no-type\"},{\"URL\":\"http://ogp.me/\",\"container-title\":\"ogp.me\",\"id\":\"https://ogp.me/\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"abstract\":\"The Open Graph protocol enables any web page to become a rich object in a social graph.\",\"title\":\"Open Graph protocol\",\"type\":\"no-type\"},{\"URL\":\"https://github.com/jgm/pandoc-citeproc/issues/308\",\"container-title\":\"GitHub\",\"id\":\"https://github.com/jgm/pandoc-citeproc/issues/308\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"abstract\":\"I want to use this citekey: [@https://ww3.cad.de/cgi-bin/ubb/forumdisplay.cgi?action=topics&amp;number=527] from this bib file: @misc{}https://ww3.cad.de/cgi-bin/ubb/forumdisplay.cgi?action=topics&a...\",\"title\":\"Url as citekey/referencekey · Issue #308 · jgm/pandoc-citeproc\",\"type\":\"no-type\"},{\"ISSN\":\"0001-0782\",\"DOI\":\"10.1145/3065386\",\"volume\":\"60\",\"URL\":\"http://doi.acm.org/10.1145/3065386\",\"page\":\"84-90\",\"container-title\":\"Commun. ACM\",\"author\":[{\"family\":\"Krizhevsky\",\"given\":\"Alex\"},{\"family\":\"Sutskever\",\"given\":\"Ilya\"},{\"family\":\"Hinton\",\"given\":\"Geoffrey E.\"}],\"id\":\"http://dl.acm.org/citation.cfm?doid=3098997.3065386\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"issued\":{\"date-parts\":[[\"2017\",\"5\"]]},\"abstract\":\"We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called \\\"dropout\\\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.\",\"title\":\"ImageNet Classification with Deep Convolutional Neural Networks\",\"type\":\"article-journal\",\"issue\":\"6\"},{\"note\":\"arXiv: 1409.1556\",\"URL\":\"http://arxiv.org/abs/1409.1556\",\"container-title\":\"arXiv:1409.1556 [cs]\",\"author\":[{\"family\":\"Simonyan\",\"given\":\"Karen\"},{\"family\":\"Zisserman\",\"given\":\"Andrew\"}],\"id\":\"https://arxiv.org/abs/1409.1556\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"issued\":{\"date-parts\":[[\"2014\",\"9\"]]},\"abstract\":\"In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.\",\"title\":\"Very Deep Convolutional Networks for Large-Scale Image Recognition\",\"type\":\"article-journal\",\"keyword\":\"Computer Science - Computer Vision and Pattern Recognition\"},{\"DOI\":\"10.1109/CVPR.2015.7298594\",\"URL\":\"https://ieeexplore.ieee.org/document/7298594\",\"page\":\"1-9\",\"container-title\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"author\":[{\"family\":\"Szegedy\",\"given\":\"C.\"},{\"literal\":\"Wei Liu\"},{\"literal\":\"Yangqing Jia\"},{\"family\":\"Sermanet\",\"given\":\"P.\"},{\"family\":\"Reed\",\"given\":\"S.\"},{\"family\":\"Anguelov\",\"given\":\"D.\"},{\"family\":\"Erhan\",\"given\":\"D.\"},{\"family\":\"Vanhoucke\",\"given\":\"V.\"},{\"family\":\"Rabinovich\",\"given\":\"A.\"}],\"id\":\"https://ieeexplore.ieee.org/document/7298594\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"issued\":{\"date-parts\":[[\"2015\",\"6\"]]},\"abstract\":\"We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.\",\"title\":\"Going deeper with convolutions\",\"type\":\"paper-conference\",\"keyword\":\"convolution, decision making, feature extraction, Hebbian learning, image classification, neural net architecture, resource allocation, convolutional neural network architecture, resource utilization, architectural decision, Hebbian principle, object classification, object detection, Computer architecture, Convolutional codes, Sparse matrices, Neural networks, Visualization, Object detection, Computer vision\"},{\"DOI\":\"10.1109/CVPR.2016.90\",\"URL\":\"https://ieeexplore.ieee.org/document/7780459\",\"page\":\"770-778\",\"container-title\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"author\":[{\"family\":\"He\",\"given\":\"K.\"},{\"family\":\"Zhang\",\"given\":\"X.\"},{\"family\":\"Ren\",\"given\":\"S.\"},{\"family\":\"Sun\",\"given\":\"J.\"}],\"id\":\"https://ieeexplore.ieee.org/document/7780459\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"issued\":{\"date-parts\":[[\"2016\",\"6\"]]},\"abstract\":\"Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.\",\"title\":\"Deep Residual Learning for Image Recognition\",\"type\":\"paper-conference\",\"keyword\":\"image classification, learning (artificial intelligence), neural nets, object detection, COCO segmentation, ImageNet localization, ILSVRC & COCO 2015 competitions, deep residual nets, COCO object detection dataset, visual recognition tasks, CIFAR-10, ILSVRC 2015 classification task, ImageNet test set, VGG nets, residual nets, ImageNet dataset, residual function learning, deeper neural network training, image recognition, deep residual learning, Training, Degradation, Complexity theory, Image recognition, Neural networks, Visualization, Image segmentation\"},{\"note\":\"original-date: 2019-08-19T22:24:50Z\",\"URL\":\"https://github.com/phiresky/pandoc-url2cite\",\"title-short\":\"Effortlessly and transparently add correctly styled citations to your markdown paper given only a URL\",\"author\":[{\"literal\":\"phiresky\"}],\"id\":\"https://github.com/phiresky/pandoc-url2cite/blob/master/example/doi-isbn.md\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"issued\":{\"date-parts\":[[\"2019\",\"12\"]]},\"title\":\"Effortlessly and transparently add correctly styled citations to your markdown paper given only a URL: Phiresky/pandoc-url2cite\",\"type\":\"no-type\"}],\"urlcolor\":\"blue\",\"date\":\"2019-12-13\",\"csl\":\"ieee-with-url.csl\",\"url2cite\":\"all-links\",\"author\":\"\\nphiresky\",\"url2cite-link-output\":\"sup\",\"title\":\"Automatic citation extraction from URLs\"},\"preview\":\"pandoc-url2cite[1] allows you to instantly and transparently cite most papers directly given only a single URL. You simply add a URL of a publication, and it will replace that with a real citation in whatever CSL[2] style you want. This means you can avoid dealing with Mendeley[3] or Zotero[4] and\",\"content_ast\":[{\"t\":\"Para\",\"c\":[{\"t\":\"Link\",\"c\":[[\"\",[],[[\"cite-meta\",\"{\\\"URL\\\":\\\"https://github.com/phiresky/pandoc-url2cite\\\",\\\"accessed\\\":{\\\"date-parts\\\":[[2019,12,14]]},\\\"author\\\":[{\\\"literal\\\":\\\"phiresky\\\"}],\\\"id\\\":\\\"https://github.com/phiresky/pandoc-url2cite\\\",\\\"issued\\\":{\\\"date-parts\\\":[[2019,12]]},\\\"note\\\":\\\"original-date: 2019-08-19T22:24:50Z\\\",\\\"title\\\":\\\"Effortlessly and transparently add correctly styled citations to your markdown paper given only a URL: Phiresky/pandoc-url2cite\\\",\\\"title-short\\\":\\\"Effortlessly and transparently add correctly styled citations to your markdown paper given only a URL\\\",\\\"type\\\":\\\"no-type\\\"}\"]]],[{\"t\":\"Str\",\"c\":\"pandoc-url2cite\"}],[\"https://github.com/phiresky/pandoc-url2cite\",\"\"]]},{\"t\":\"Superscript\",\"c\":[{\"t\":\"Cite\",\"c\":[[{\"citationSuffix\":[],\"citationNoteNum\":0,\"citationMode\":{\"t\":\"NormalCitation\"},\"citationPrefix\":[],\"citationId\":\"https://github.com/phiresky/pandoc-url2cite\",\"citationHash\":1}],[{\"t\":\"Str\",\"c\":\"[\"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"1\"}],[\"#ref-https://github.com/phiresky/pandoc-url2cite\",\"\"]]},{\"t\":\"Str\",\"c\":\"]\"}]]}]},{\"t\":\"Str\",\"c\":\" allows you to instantly and transparently cite most papers directly given only a single URL.\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"You simply add a URL of a publication, and it will replace that with a real citation in whatever \"},{\"t\":\"Link\",\"c\":[[\"\",[],[[\"cite-meta\",\"{\\\"URL\\\":\\\"https://citationstyles.org/\\\",\\\"abstract\\\":\\\"CitationStyles.org, home of the Citation Style Language (CSL), a popular open XML-based language to describe the formatting of citations and bibliographies.\\\",\\\"accessed\\\":{\\\"date-parts\\\":[[2019,12,14]]},\\\"author\\\":[{\\\"family\\\":\\\"Name\\\",\\\"given\\\":\\\"Your\\\"}],\\\"container-title\\\":\\\"Citation Style Language\\\",\\\"id\\\":\\\"https://citationstyles.org/\\\",\\\"title\\\":\\\"Citation Style Language\\\",\\\"type\\\":\\\"no-type\\\"}\"]]],[{\"t\":\"Str\",\"c\":\"CSL\"}],[\"https://citationstyles.org/\",\"\"]]},{\"t\":\"Superscript\",\"c\":[{\"t\":\"Cite\",\"c\":[[{\"citationSuffix\":[],\"citationNoteNum\":0,\"citationMode\":{\"t\":\"NormalCitation\"},\"citationPrefix\":[],\"citationId\":\"https://citationstyles.org/\",\"citationHash\":2}],[{\"t\":\"Str\",\"c\":\"[\"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"2\"}],[\"#ref-https://citationstyles.org/\",\"\"]]},{\"t\":\"Str\",\"c\":\"]\"}]]}]},{\"t\":\"Str\",\"c\":\" style you want. This means you can avoid dealing with \"},{\"t\":\"Link\",\"c\":[[\"\",[],[[\"cite-meta\",\"{\\\"URL\\\":\\\"https://www.mendeley.com/?interaction_required=true\\\",\\\"abstract\\\":\\\"Mendeley is a free reference manager and an academic social network. Manage your research, showcase your work, connect and collaborate with over five million researchers worldwide.\\\",\\\"accessed\\\":{\\\"date-parts\\\":[[2019,12,14]]},\\\"container-title\\\":\\\"www.mendeley.com\\\",\\\"id\\\":\\\"https://www.mendeley.com/\\\",\\\"title\\\":\\\"Mendeley - Reference Management Software & Researcher Network\\\",\\\"type\\\":\\\"no-type\\\"}\"]]],[{\"t\":\"Str\",\"c\":\"Mendeley\"}],[\"https://www.mendeley.com/\",\"\"]]},{\"t\":\"Superscript\",\"c\":[{\"t\":\"Cite\",\"c\":[[{\"citationSuffix\":[],\"citationNoteNum\":0,\"citationMode\":{\"t\":\"NormalCitation\"},\"citationPrefix\":[],\"citationId\":\"https://www.mendeley.com/\",\"citationHash\":3}],[{\"t\":\"Str\",\"c\":\"[\"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"3\"}],[\"#ref-https://www.mendeley.com/\",\"\"]]},{\"t\":\"Str\",\"c\":\"]\"}]]}]},{\"t\":\"Str\",\"c\":\" or \"},{\"t\":\"Link\",\"c\":[[\"\",[],[[\"cite-meta\",\"{\\\"URL\\\":\\\"https://www.zotero.org/\\\",\\\"abstract\\\":\\\"Zotero is a free, easy-to-use tool to help you collect, organize, cite, and share research.\\\",\\\"accessed\\\":{\\\"date-parts\\\":[[2019,12,14]]},\\\"container-title\\\":\\\"www.zotero.org\\\",\\\"id\\\":\\\"https://www.zotero.org/\\\",\\\"title\\\":\\\"Zotero Your personal research assistant\\\",\\\"type\\\":\\\"no-type\\\"}\"]]],[{\"t\":\"Str\",\"c\":\"Zotero\"}],[\"https://www.zotero.org/\",\"\"]]},{\"t\":\"Superscript\",\"c\":[{\"t\":\"Cite\",\"c\":[[{\"citationSuffix\":[],\"citationNoteNum\":0,\"citationMode\":{\"t\":\"NormalCitation\"},\"citationPrefix\":[],\"citationId\":\"https://www.zotero.org/\",\"citationHash\":4}],[{\"t\":\"Str\",\"c\":\"[\"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"4\"}],[\"#ref-https://www.zotero.org/\",\"\"]]},{\"t\":\"Str\",\"c\":\"]\"}]]}]},{\"t\":\"Str\",\"c\":\" and keeping your Reference Manager database and bibtex file in sync, especially when collaborating with others.\"}]},{\"t\":\"Header\",\"c\":[1,[\"minimal-example\",[],[]],[{\"t\":\"Str\",\"c\":\"Minimal Example\"}]]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Here is a minimal example:\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Strong\",\"c\":[{\"t\":\"Str\",\"c\":\"minimal.md\"}]}]},{\"t\":\"CodeBlock\",\"c\":[[\"\",[\"markdown\",\"number-lines\"],[]],\"# Introduction\\n\\nThe GAN was first introduced in [@gan].\\n\\n# References\\n\\n[@gan]: https://papers.nips.cc/paper/5423-generative-adversarial-nets\"]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Compiling this file with this command\"}]},{\"t\":\"CodeBlock\",\"c\":[[\"\",[\"bash\"],[]],\"pandoc \\\\\\n    --filter=pandoc-url2cite --filter=pandoc-citeproc \\\\\\n    --csl ieee-with-url.csl \\\\\\n    minimal.md -o minimal.pdf\"]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"This results in the following output:\"},{\"t\":\"LineBreak\"},{\"t\":\"Strong\",\"c\":[{\"t\":\"Str\",\"c\":\"minimal.pdf\"}]},{\"t\":\"LineBreak\"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Image\",\"c\":[[\"\",[],[]],[],[\"https://github.com/phiresky/pandoc-url2cite/raw/master/example/minimal.png\",\"\"]]}],[\"https://github.com/phiresky/pandoc-url2cite/blob/master/example/minimal.pdf\",\"no-url2cite\"]]}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"For a longer example, you can look at the source of this file itself, which is both \"},{\"t\":\"Link\",\"c\":[[\"\",[],[[\"cite-meta\",\"{\\\"URL\\\":\\\"https://phiresky.github.io/blog/2019/pandoc-url2cite/\\\",\\\"abstract\\\":\\\"pandoc-url2cite[1] allows you to instantly and transparently cite most papers directly given only a single URL. You simply add a URL of a publication, and it will replace that with a real citation in whatever CSL[2] style you want. This means you can avoid dealing with Mendeley[3] or Zotero[4] and\\\",\\\"accessed\\\":{\\\"date-parts\\\":[[2019,12,14]]},\\\"container-title\\\":\\\"phiresky.github.io\\\",\\\"id\\\":\\\"https://phiresky.github.io/blog/2019/pandoc-url2cite/\\\",\\\"title\\\":\\\"Automatic citation extraction from URLs (draft) - phiresky’s blog\\\",\\\"type\\\":\\\"no-type\\\"}\"]]],[{\"t\":\"Str\",\"c\":\"a blog post\"}],[\"https://phiresky.github.io/blog/2019/pandoc-url2cite/\",\"\"]]},{\"t\":\"Superscript\",\"c\":[{\"t\":\"Cite\",\"c\":[[{\"citationSuffix\":[],\"citationNoteNum\":0,\"citationMode\":{\"t\":\"NormalCitation\"},\"citationPrefix\":[],\"citationId\":\"https://phiresky.github.io/blog/2019/pandoc-url2cite/\",\"citationHash\":5}],[{\"t\":\"Str\",\"c\":\"[\"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"5\"}],[\"#ref-https://phiresky.github.io/blog/2019/pandoc-url2cite/\",\"\"]]},{\"t\":\"Str\",\"c\":\"]\"}]]}]},{\"t\":\"Str\",\"c\":\", GitHub Readme and LaTeX \"},{\"t\":\"Quoted\",\"c\":[{\"t\":\"DoubleQuote\"},[{\"t\":\"Str\",\"c\":\"paper\"}]]},{\"t\":\"Str\",\"c\":\":\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Strong\",\"c\":[{\"t\":\"Str\",\"c\":\"README.pdf\"}]},{\"t\":\"LineBreak\"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Image\",\"c\":[[\"\",[],[]],[],[\"https://github.com/phiresky/pandoc-url2cite/raw/master/example/readme.png\",\"\"]]}],[\"https://github.com/phiresky/pandoc-url2cite/blob/master/README.pdf\",\"no-url2cite\"]]}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"Source README.md\"}],[\"https://raw.githubusercontent.com/phiresky/pandoc-url2cite/master/README.md\",\"no-url2cite\"]]},{\"t\":\"Str\",\"c\":\" - \"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"Result README.pdf\"}],[\"https://github.com/phiresky/pandoc-url2cite/blob/master/README.pdf\",\"no-url2cite\"]]}]},{\"t\":\"Header\",\"c\":[1,[\"how-to-use\",[],[]],[{\"t\":\"Str\",\"c\":\"How to Use\"}]]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Install this package globally using \"},{\"t\":\"Code\",\"c\":[[\"\",[],[]],\"npm install -g pandoc-url2cite\"]},{\"t\":\"Str\",\"c\":\".\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Then, add \"},{\"t\":\"Code\",\"c\":[[\"\",[],[]],\"--filter=pandoc-url2cite\"]},{\"t\":\"Str\",\"c\":\" to your pandoc command (before pandoc-citeproc, see the minimal example above).\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Alternatively, clone \"},{\"t\":\"Link\",\"c\":[[\"\",[],[[\"cite-meta\",\"{\\\"URL\\\":\\\"https://github.com/phiresky/pandoc-url2cite\\\",\\\"accessed\\\":{\\\"date-parts\\\":[[2019,12,14]]},\\\"author\\\":[{\\\"literal\\\":\\\"phiresky\\\"}],\\\"id\\\":\\\"https://github.com/phiresky/pandoc-url2cite\\\",\\\"issued\\\":{\\\"date-parts\\\":[[2019,12]]},\\\"note\\\":\\\"original-date: 2019-08-19T22:24:50Z\\\",\\\"title\\\":\\\"Effortlessly and transparently add correctly styled citations to your markdown paper given only a URL: Phiresky/pandoc-url2cite\\\",\\\"title-short\\\":\\\"Effortlessly and transparently add correctly styled citations to your markdown paper given only a URL\\\",\\\"type\\\":\\\"no-type\\\"}\"]]],[{\"t\":\"Str\",\"c\":\"this repo\"}],[\"https://github.com/phiresky/pandoc-url2cite\",\"\"]]},{\"t\":\"Superscript\",\"c\":[{\"t\":\"Cite\",\"c\":[[{\"citationSuffix\":[],\"citationNoteNum\":0,\"citationMode\":{\"t\":\"NormalCitation\"},\"citationPrefix\":[],\"citationId\":\"https://github.com/phiresky/pandoc-url2cite\",\"citationHash\":6}],[{\"t\":\"Str\",\"c\":\"[\"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"1\"}],[\"#ref-https://github.com/phiresky/pandoc-url2cite\",\"\"]]},{\"t\":\"Str\",\"c\":\"]\"}]]}]},{\"t\":\"Str\",\"c\":\" somewhere, then install the dependencies using \"},{\"t\":\"Code\",\"c\":[[\"\",[],[]],\"npm ci install\"]},{\"t\":\"Str\",\"c\":\".\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"If you’re not familiar with writing papers in pandoc, you can refer to \"},{\"t\":\"Link\",\"c\":[[\"\",[],[[\"cite-meta\",\"{\\\"URL\\\":\\\"https://opensource.com/article/18/9/pandoc-research-paper\\\",\\\"abstract\\\":\\\"Learn how to manage section references, figures, tables, and more in Markdown.\\\",\\\"accessed\\\":{\\\"date-parts\\\":[[2019,12,14]]},\\\"author\\\":[{\\\"family\\\":\\\"Kiko\\\",\\\"given\\\":\\\"Fernandez-Reyes\\\"}],\\\"container-title\\\":\\\"Opensource.com\\\",\\\"id\\\":\\\"https://opensource.com/article/18/9/pandoc-research-paper\\\",\\\"title\\\":\\\"How to use Pandoc to produce a research paper\\\",\\\"type\\\":\\\"no-type\\\"}\"]]],[{\"t\":\"Str\",\"c\":\"e.g. this article\"}],[\"https://opensource.com/article/18/9/pandoc-research-paper\",\"\"]]},{\"t\":\"Superscript\",\"c\":[{\"t\":\"Cite\",\"c\":[[{\"citationSuffix\":[],\"citationNoteNum\":0,\"citationMode\":{\"t\":\"NormalCitation\"},\"citationPrefix\":[],\"citationId\":\"https://opensource.com/article/18/9/pandoc-research-paper\",\"citationHash\":7}],[{\"t\":\"Str\",\"c\":\"[\"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"6\"}],[\"#ref-https://opensource.com/article/18/9/pandoc-research-paper\",\"\"]]},{\"t\":\"Str\",\"c\":\"]\"}]]}]},{\"t\":\"Str\",\"c\":\". It’s pretty flexible, you can use templates from whatever conference you want, and you can still use inline latex code if you need it (and you are ok with not being able to convert your document to nice HTML or EPUB anymore).\"}]},{\"t\":\"Header\",\"c\":[2,[\"citation-syntax\",[],[]],[{\"t\":\"Str\",\"c\":\"Citation Syntax\"}]]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"url2cite allows multiple ways to cite:\"}]},{\"t\":\"OrderedList\",\"c\":[[1,{\"t\":\"Decimal\"},{\"t\":\"Period\"}],[[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"(PREFERRED) Use the pandoc citation syntax for citations:\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Code\",\"c\":[[\"\",[],[]],\"The authors of [@alexnet] first introduced CNNs to the ImageNet challenge.\"]}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"More information about referencing specific pages etc. is in the \"},{\"t\":\"Link\",\"c\":[[\"\",[],[[\"cite-meta\",\"{\\\"URL\\\":\\\"https://pandoc.org/MANUAL.html#citations\\\",\\\"accessed\\\":{\\\"date-parts\\\":[[2019,12,14]]},\\\"container-title\\\":\\\"pandoc.org\\\",\\\"id\\\":\\\"https://pandoc.org/MANUAL.html#citations\\\",\\\"title\\\":\\\"Pandoc - Pandoc User’s Guide\\\",\\\"type\\\":\\\"no-type\\\"}\"]]],[{\"t\":\"Str\",\"c\":\"pandoc manual\"}],[\"https://pandoc.org/MANUAL.html#citations\",\"\"]]},{\"t\":\"Superscript\",\"c\":[{\"t\":\"Cite\",\"c\":[[{\"citationSuffix\":[],\"citationNoteNum\":0,\"citationMode\":{\"t\":\"NormalCitation\"},\"citationPrefix\":[],\"citationId\":\"https://pandoc.org/MANUAL.html#citations\",\"citationHash\":8}],[{\"t\":\"Str\",\"c\":\"[\"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"7\"}],[\"#ref-https://pandoc.org/MANUAL.html#citations\",\"\"]]},{\"t\":\"Str\",\"c\":\"]\"}]]}]},{\"t\":\"Str\",\"c\":\".\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Then add the URLs with the usual \"},{\"t\":\"Quoted\",\"c\":[{\"t\":\"DoubleQuote\"},[{\"t\":\"Str\",\"c\":\"link reference\"}]]},{\"t\":\"Str\",\"c\":\" syntax to the bottom of your document in its own paragraph:\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Code\",\"c\":[[\"\",[],[]],\"[@alexnet]: https://...\"]}]}],[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Convert all links to citations\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Add \"},{\"t\":\"Code\",\"c\":[[\"\",[],[]],\"url2cite: all-links\"]},{\"t\":\"Str\",\"c\":\" to your \"},{\"t\":\"Link\",\"c\":[[\"\",[],[[\"cite-meta\",\"{\\\"URL\\\":\\\"https://pandoc.org/MANUAL.html#extension-yaml_metadata_block\\\",\\\"accessed\\\":{\\\"date-parts\\\":[[2019,12,14]]},\\\"container-title\\\":\\\"pandoc.org\\\",\\\"id\\\":\\\"https://pandoc.org/MANUAL.html#extension-yaml_metadata_block\\\",\\\"title\\\":\\\"Pandoc - Pandoc User’s Guide\\\",\\\"type\\\":\\\"no-type\\\"}\"]]],[{\"t\":\"Str\",\"c\":\"yaml front matter\"}],[\"https://pandoc.org/MANUAL.html#extension-yaml_metadata_block\",\"\"]]},{\"t\":\"Superscript\",\"c\":[{\"t\":\"Cite\",\"c\":[[{\"citationSuffix\":[],\"citationNoteNum\":0,\"citationMode\":{\"t\":\"NormalCitation\"},\"citationPrefix\":[],\"citationId\":\"https://pandoc.org/MANUAL.html#extension-yaml_metadata_block\",\"citationHash\":9}],[{\"t\":\"Str\",\"c\":\"[\"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"8\"}],[\"#ref-https://pandoc.org/MANUAL.html#extension-yaml_metadata_block\",\"\"]]},{\"t\":\"Str\",\"c\":\"]\"}]]}]},{\"t\":\"Str\",\"c\":\". This will cause all links in the document to be converted to references.\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"You can still blacklist some links by adding \"},{\"t\":\"Code\",\"c\":[[\"\",[],[]],\"no-url2cite\"]},{\"t\":\"Str\",\"c\":\" to either the CSS class of the link (pandoc-only):\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Code\",\"c\":[[\"\",[],[]],\"[foo](http://example.com){.no-url2cite}\"]}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"or to the link title:\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Code\",\"c\":[[\"\",[],[]],\"[foo](http://example.com \\\"no-url2cite\\\")\"]},{\"t\":\"Str\",\"c\":\".\"}]}]]]},{\"t\":\"Header\",\"c\":[1,[\"how-it-works\",[],[]],[{\"t\":\"Str\",\"c\":\"How it Works\"}]]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"The main idea is that usually every piece of research you might want to cite is fully identifiable by an URL - no need to manually enter metadata like author, release date, journal, etc. Citation managers like Zotero already use this and enable you to automatically fetch metadata from a website. But then you still have a citation database somewhere that you may or may not be able to synchronize with different computers, but probably won’t be able to add to the version control of your paper. There’s hacks such as \"},{\"t\":\"Link\",\"c\":[[\"\",[],[[\"cite-meta\",\"{\\\"URL\\\":\\\"https://github.com/retorquere/zotero-better-bibtex\\\",\\\"accessed\\\":{\\\"date-parts\\\":[[2019,12,14]]},\\\"author\\\":[{\\\"family\\\":\\\"Heyns\\\",\\\"given\\\":\\\"Emiliano\\\"}],\\\"id\\\":\\\"https://github.com/retorquere/zotero-better-bibtex\\\",\\\"issued\\\":{\\\"date-parts\\\":[[2019,12]]},\\\"note\\\":\\\"original-date: 2013-09-20T08:26:14Z\\\",\\\"title\\\":\\\"Make Zotero effective for us LaTeX holdouts. Contribute to retorquere/zotero-better-bibtex development by creating an account on GitHub\\\",\\\"type\\\":\\\"no-type\\\"}\"]]],[{\"t\":\"Str\",\"c\":\"better-bibtex\"}],[\"https://github.com/retorquere/zotero-better-bibtex\",\"\"]]},{\"t\":\"Superscript\",\"c\":[{\"t\":\"Cite\",\"c\":[[{\"citationSuffix\":[],\"citationNoteNum\":0,\"citationMode\":{\"t\":\"NormalCitation\"},\"citationPrefix\":[],\"citationId\":\"https://github.com/retorquere/zotero-better-bibtex\",\"citationHash\":10}],[{\"t\":\"Str\",\"c\":\"[\"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"9\"}],[\"#ref-https://github.com/retorquere/zotero-better-bibtex\",\"\"]]},{\"t\":\"Str\",\"c\":\"]\"}]]}]},{\"t\":\"Str\",\"c\":\" to automatically generate and update diffable bibtex files – But that means you now have two sources of truth, and since the export is one-way this leads to multiple contributors overriding each other’s changes. pandoc-url2cite goes a step further: URLs are directly used as the cite keys, and the \"},{\"t\":\"Quoted\",\"c\":[{\"t\":\"DoubleQuote\"},[{\"t\":\"Str\",\"c\":\"bibliography file\"}]]},{\"t\":\"Str\",\"c\":\" is just an auto-generated intermediary artifact of those URLs.\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"pandoc-url2cite is based on the work of the \"},{\"t\":\"Link\",\"c\":[[\"\",[],[[\"cite-meta\",\"{\\\"URL\\\":\\\"https://www.zotero.org/\\\",\\\"abstract\\\":\\\"Zotero is a free, easy-to-use tool to help you collect, organize, cite, and share research.\\\",\\\"accessed\\\":{\\\"date-parts\\\":[[2019,12,14]]},\\\"container-title\\\":\\\"www.zotero.org\\\",\\\"id\\\":\\\"https://www.zotero.org/\\\",\\\"title\\\":\\\"Zotero Your personal research assistant\\\",\\\"type\\\":\\\"no-type\\\"}\"]]],[{\"t\":\"Str\",\"c\":\"Zotero\"}],[\"https://www.zotero.org/\",\"\"]]},{\"t\":\"Superscript\",\"c\":[{\"t\":\"Cite\",\"c\":[[{\"citationSuffix\":[],\"citationNoteNum\":0,\"citationMode\":{\"t\":\"NormalCitation\"},\"citationPrefix\":[],\"citationId\":\"https://www.zotero.org/\",\"citationHash\":11}],[{\"t\":\"Str\",\"c\":\"[\"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"4\"}],[\"#ref-https://www.zotero.org/\",\"\"]]},{\"t\":\"Str\",\"c\":\"]\"}]]}]},{\"t\":\"Str\",\"c\":\" developers. Zotero has a set of \"},{\"t\":\"Link\",\"c\":[[\"\",[],[[\"cite-meta\",\"{\\\"URL\\\":\\\"https://www.zotero.org/support/dev/translators\\\",\\\"abstract\\\":\\\"Zotero is a free, easy-to-use tool to help you collect, organize, cite, and share research.\\\",\\\"accessed\\\":{\\\"date-parts\\\":[[2019,12,14]]},\\\"container-title\\\":\\\"www.zotero.org\\\",\\\"id\\\":\\\"https://www.zotero.org/support/dev/translators\\\",\\\"title\\\":\\\"Dev:Translators [Zotero Documentation]\\\",\\\"title-short\\\":\\\"Dev\\\",\\\"type\\\":\\\"no-type\\\"}\"]]],[{\"t\":\"Quoted\",\"c\":[{\"t\":\"DoubleQuote\"},[{\"t\":\"Str\",\"c\":\"Translators\"}]]}],[\"https://www.zotero.org/support/dev/translators\",\"\"]]},{\"t\":\"Superscript\",\"c\":[{\"t\":\"Cite\",\"c\":[[{\"citationSuffix\":[],\"citationNoteNum\":0,\"citationMode\":{\"t\":\"NormalCitation\"},\"citationPrefix\":[],\"citationId\":\"https://www.zotero.org/support/dev/translators\",\"citationHash\":12}],[{\"t\":\"Str\",\"c\":\"[\"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"10\"}],[\"#ref-https://www.zotero.org/support/dev/translators\",\"\"]]},{\"t\":\"Str\",\"c\":\"]\"}]]}]},{\"t\":\"Str\",\"c\":\" that are able to extract citation info from a number of specific and general web pages. These translators are written in Javascript and run within the context of the given web site. They are made to be used from the Zotero Connector browser extension, but thankfully there is a standalone \"},{\"t\":\"Link\",\"c\":[[\"\",[],[[\"cite-meta\",\"{\\\"URL\\\":\\\"https://github.com/zotero/translation-server\\\",\\\"accessed\\\":{\\\"date-parts\\\":[[2019,12,14]]},\\\"id\\\":\\\"https://github.com/zotero/translation-server\\\",\\\"issued\\\":{\\\"date-parts\\\":[[2019,12]]},\\\"note\\\":\\\"original-date: 2018-06-11T11:28:53Z\\\",\\\"publisher\\\":\\\"zotero\\\",\\\"title\\\":\\\"A Node.Js-based server to run Zotero translators. Contribute to zotero/translation-server development by creating an account on GitHub\\\",\\\"type\\\":\\\"no-type\\\"}\"]]],[{\"t\":\"Str\",\"c\":\"Translation Server\"}],[\"https://github.com/zotero/translation-server\",\"\"]]},{\"t\":\"Superscript\",\"c\":[{\"t\":\"Cite\",\"c\":[[{\"citationSuffix\":[],\"citationNoteNum\":0,\"citationMode\":{\"t\":\"NormalCitation\"},\"citationPrefix\":[],\"citationId\":\"https://github.com/zotero/translation-server\",\"citationHash\":13}],[{\"t\":\"Str\",\"c\":\"[\"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"11\"}],[\"#ref-https://github.com/zotero/translation-server\",\"\"]]},{\"t\":\"Str\",\"c\":\"]\"}]]}]},{\"t\":\"Str\",\"c\":\" as well. To avoid the effort required to automatically start and manage this server locally, pandoc-url2cite instead uses a publicly accessible instance of this server provided by Wikipedia with a \"},{\"t\":\"Link\",\"c\":[[\"\",[],[[\"cite-meta\",\"{\\\"URL\\\":\\\"https://www.mediawiki.org/wiki/Citoid/API\\\",\\\"accessed\\\":{\\\"date-parts\\\":[[2019,12,14]]},\\\"container-title\\\":\\\"www.mediawiki.org\\\",\\\"id\\\":\\\"https://www.mediawiki.org/wiki/Citoid/API\\\",\\\"title\\\":\\\"Citoid/API - MediaWiki\\\",\\\"type\\\":\\\"no-type\\\"}\"]]],[{\"t\":\"Str\",\"c\":\"public REST API\"}],[\"https://www.mediawiki.org/wiki/Citoid/API\",\"\"]]},{\"t\":\"Superscript\",\"c\":[{\"t\":\"Cite\",\"c\":[[{\"citationSuffix\":[],\"citationNoteNum\":0,\"citationMode\":{\"t\":\"NormalCitation\"},\"citationPrefix\":[],\"citationId\":\"https://www.mediawiki.org/wiki/Citoid/API\",\"citationHash\":14}],[{\"t\":\"Str\",\"c\":\"[\"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"12\"}],[\"#ref-https://www.mediawiki.org/wiki/Citoid/API\",\"\"]]},{\"t\":\"Str\",\"c\":\"]\"}]]}]},{\"t\":\"Str\",\"c\":\".\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"All citation data is cached (permanently) as bibtex as well as CSL to \"},{\"t\":\"Code\",\"c\":[[\"\",[],[]],\"citation-cache.json\"]},{\"t\":\"Str\",\"c\":\". This is both to improve performance and to make sure references stay the same forever after the initial fetch, as well as to avoid problems if the API might be down in the future. This also means that errors in the citation data can be fixed manually, although if you find you need to do a lot of manual tweaking you might again be better off with Zotero.\"}]},{\"t\":\"Header\",\"c\":[1,[\"limitations\",[],[]],[{\"t\":\"Str\",\"c\":\"Limitations\"}]]},{\"t\":\"OrderedList\",\"c\":[[1,{\"t\":\"Decimal\"},{\"t\":\"Period\"}],[[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Currently, extracting the metadata from direct URLs of full text PDFs does not work, so you will need to use the URL of an overview / abstract page etc. I’m not sure why, since this does work in Zotero. \"},{\"t\":\"Link\",\"c\":[[\"\",[],[[\"cite-meta\",\"{\\\"URL\\\":\\\"https://github.com/zotero/translation-server/issues/70\\\",\\\"abstract\\\":\\\"Related to #38, but a few translators are able to function based on the URL, even when it&#39;s a PDF page. We should try to support those cases, before either trying PDF recognition (from #38) or ...\\\",\\\"accessed\\\":{\\\"date-parts\\\":[[2019,12,14]]},\\\"container-title\\\":\\\"GitHub\\\",\\\"id\\\":\\\"https://github.com/zotero/translation-server/issues/70\\\",\\\"title\\\":\\\"Try translating PDF URLs based on URL · Issue #70 · zotero/translation-server\\\",\\\"type\\\":\\\"no-type\\\"}\"]]],[{\"t\":\"Str\",\"c\":\"More info might be here\"}],[\"https://github.com/zotero/translation-server/issues/70\",\"\"]]},{\"t\":\"Superscript\",\"c\":[{\"t\":\"Cite\",\"c\":[[{\"citationSuffix\":[],\"citationNoteNum\":0,\"citationMode\":{\"t\":\"NormalCitation\"},\"citationPrefix\":[],\"citationId\":\"https://github.com/zotero/translation-server/issues/70\",\"citationHash\":15}],[{\"t\":\"Str\",\"c\":\"[\"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"13\"}],[\"#ref-https://github.com/zotero/translation-server/issues/70\",\"\"]]},{\"t\":\"Str\",\"c\":\"]\"}]]}]},{\"t\":\"Str\",\"c\":\".\"}]}],[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Currently, this filter only works if you use pandoc-citeproc, because the citations are written directly into the document metadata instead of into a bibtex file. If you want to use natbib or biblatex for citations, this filter currently won’t work. Using citeproc has the disadvantage that it is somewhat less configurable than the \"},{\"t\":\"Quoted\",\"c\":[{\"t\":\"DoubleQuote\"},[{\"t\":\"Str\",\"c\":\"real\"}]]},{\"t\":\"Str\",\"c\":\" LaTeX citation text generators and the CSL language has some limitations. For example, the \"},{\"t\":\"Link\",\"c\":[[\"\",[],[[\"cite-meta\",\"{\\\"URL\\\":\\\"https://www.overleaf.com/learn/latex/Bibtex_bibliography_styles\\\",\\\"abstract\\\":\\\"An online LaTeX editor that’s easy to use. No installation, real-time collaboration, version control, hundreds of LaTeX templates, and more.\\\",\\\"accessed\\\":{\\\"date-parts\\\":[[2019,12,14]]},\\\"container-title\\\":\\\"www.overleaf.com\\\",\\\"id\\\":\\\"https://www.overleaf.com/learn/latex/Bibtex_bibliography_styles\\\",\\\"title\\\":\\\"Bibtex bibliography styles\\\",\\\"type\\\":\\\"no-type\\\"}\"]]],[{\"t\":\"Str\",\"c\":\"bibtex \"},{\"t\":\"Quoted\",\"c\":[{\"t\":\"DoubleQuote\"},[{\"t\":\"Str\",\"c\":\"alpha\"}]]}],[\"https://www.overleaf.com/learn/latex/Bibtex_bibliography_styles\",\"\"]]},{\"t\":\"Superscript\",\"c\":[{\"t\":\"Cite\",\"c\":[[{\"citationSuffix\":[],\"citationNoteNum\":0,\"citationMode\":{\"t\":\"NormalCitation\"},\"citationPrefix\":[],\"citationId\":\"https://www.overleaf.com/learn/latex/Bibtex_bibliography_styles\",\"citationHash\":16}],[{\"t\":\"Str\",\"c\":\"[\"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"14\"}],[\"#ref-https://www.overleaf.com/learn/latex/Bibtex_bibliography_styles\",\"\"]]},{\"t\":\"Str\",\"c\":\"]\"}]]}]},{\"t\":\"Str\",\"c\":\" style sometimes used in Germany can’t be described in CSL.\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"To make it work with biblatex, this script would need to write out a *.bib file somewhere temporarily and reference that in the latex code.\"}]}],[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Some websites just have wrong meta information. For example, citationstyles.org has set \"},{\"t\":\"Quoted\",\"c\":[{\"t\":\"DoubleQuote\"},[{\"t\":\"Str\",\"c\":\"Your Name\"}]]},{\"t\":\"Str\",\"c\":\" as the website author in their \"},{\"t\":\"Link\",\"c\":[[\"\",[],[[\"cite-meta\",\"{\\\"URL\\\":\\\"http://ogp.me/\\\",\\\"abstract\\\":\\\"The Open Graph protocol enables any web page to become a rich object in a social graph.\\\",\\\"accessed\\\":{\\\"date-parts\\\":[[2019,12,14]]},\\\"container-title\\\":\\\"ogp.me\\\",\\\"id\\\":\\\"https://ogp.me/\\\",\\\"title\\\":\\\"Open Graph protocol\\\",\\\"type\\\":\\\"no-type\\\"}\"]]],[{\"t\":\"Str\",\"c\":\"Open Graph\"}],[\"https://ogp.me/\",\"\"]]},{\"t\":\"Superscript\",\"c\":[{\"t\":\"Cite\",\"c\":[[{\"citationSuffix\":[],\"citationNoteNum\":0,\"citationMode\":{\"t\":\"NormalCitation\"},\"citationPrefix\":[],\"citationId\":\"https://ogp.me/\",\"citationHash\":17}],[{\"t\":\"Str\",\"c\":\"[\"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"15\"}],[\"#ref-https://ogp.me/\",\"\"]]},{\"t\":\"Str\",\"c\":\"]\"}]]}]},{\"t\":\"Str\",\"c\":\" metadata.\"}]}],[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Using URLs directly as citekeys (e.g. \"},{\"t\":\"Code\",\"c\":[[\"\",[],[]],\"[@https://google.com]\"]},{\"t\":\"Str\",\"c\":\" does not work because of pandoc parsing, see \"},{\"t\":\"Link\",\"c\":[[\"\",[],[[\"cite-meta\",\"{\\\"URL\\\":\\\"https://github.com/jgm/pandoc-citeproc/issues/308\\\",\\\"abstract\\\":\\\"I want to use this citekey: [@https://ww3.cad.de/cgi-bin/ubb/forumdisplay.cgi?action=topics&amp;number=527] from this bib file: @misc{}https://ww3.cad.de/cgi-bin/ubb/forumdisplay.cgi?action=topics&a...\\\",\\\"accessed\\\":{\\\"date-parts\\\":[[2019,12,14]]},\\\"container-title\\\":\\\"GitHub\\\",\\\"id\\\":\\\"https://github.com/jgm/pandoc-citeproc/issues/308\\\",\\\"title\\\":\\\"Url as citekey/referencekey · Issue #308 · jgm/pandoc-citeproc\\\",\\\"type\\\":\\\"no-type\\\"}\"]]],[{\"t\":\"Str\",\"c\":\"this issue\"}],[\"https://github.com/jgm/pandoc-citeproc/issues/308\",\"\"]]},{\"t\":\"Superscript\",\"c\":[{\"t\":\"Cite\",\"c\":[[{\"citationSuffix\":[],\"citationNoteNum\":0,\"citationMode\":{\"t\":\"NormalCitation\"},\"citationPrefix\":[],\"citationId\":\"https://github.com/jgm/pandoc-citeproc/issues/308\",\"citationHash\":18}],[{\"t\":\"Str\",\"c\":\"[\"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"16\"}],[\"#ref-https://github.com/jgm/pandoc-citeproc/issues/308\",\"\"]]},{\"t\":\"Str\",\"c\":\"]\"}]]}]},{\"t\":\"Str\",\"c\":\". But it does work for ISBNs and DOIs:\"}]},{\"t\":\"CodeBlock\",\"c\":[[\"\",[],[]],\"The book [@isbn:978-0374533557, pp. 15-17] is interesting.\"]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"See \"},{\"t\":\"Link\",\"c\":[[\"\",[],[[\"cite-meta\",\"{\\\"URL\\\":\\\"https://github.com/phiresky/pandoc-url2cite\\\",\\\"accessed\\\":{\\\"date-parts\\\":[[2019,12,14]]},\\\"author\\\":[{\\\"literal\\\":\\\"phiresky\\\"}],\\\"id\\\":\\\"https://github.com/phiresky/pandoc-url2cite/blob/master/example/doi-isbn.md\\\",\\\"issued\\\":{\\\"date-parts\\\":[[2019,12]]},\\\"note\\\":\\\"original-date: 2019-08-19T22:24:50Z\\\",\\\"title\\\":\\\"Effortlessly and transparently add correctly styled citations to your markdown paper given only a URL: Phiresky/pandoc-url2cite\\\",\\\"title-short\\\":\\\"Effortlessly and transparently add correctly styled citations to your markdown paper given only a URL\\\",\\\"type\\\":\\\"no-type\\\"}\"]]],[{\"t\":\"Str\",\"c\":\"this example\"}],[\"https://github.com/phiresky/pandoc-url2cite/blob/master/example/doi-isbn.md\",\"\"]]},{\"t\":\"Superscript\",\"c\":[{\"t\":\"Cite\",\"c\":[[{\"citationSuffix\":[],\"citationNoteNum\":0,\"citationMode\":{\"t\":\"NormalCitation\"},\"citationPrefix\":[],\"citationId\":\"https://github.com/phiresky/pandoc-url2cite/blob/master/example/doi-isbn.md\",\"citationHash\":19}],[{\"t\":\"Str\",\"c\":\"[\"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"17\"}],[\"#ref-https://github.com/phiresky/pandoc-url2cite/blob/master/example/doi-isbn.md\",\"\"]]},{\"t\":\"Str\",\"c\":\"]\"}]]}]}]}]]]},{\"t\":\"Header\",\"c\":[1,[\"related-work-longer-example\",[],[]],[{\"t\":\"Str\",\"c\":\"Related Work (Longer Example)\"}]]},{\"t\":\"Para\",\"c\":[{\"t\":\"Link\",\"c\":[[\"\",[],[[\"cite-meta\",\"{\\\"DOI\\\":\\\"10.1145/3065386\\\",\\\"ISSN\\\":\\\"0001-0782\\\",\\\"URL\\\":\\\"http://doi.acm.org/10.1145/3065386\\\",\\\"abstract\\\":\\\"We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called \\\\\\\"dropout\\\\\\\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.\\\",\\\"accessed\\\":{\\\"date-parts\\\":[[2019,12,14]]},\\\"author\\\":[{\\\"family\\\":\\\"Krizhevsky\\\",\\\"given\\\":\\\"Alex\\\"},{\\\"family\\\":\\\"Sutskever\\\",\\\"given\\\":\\\"Ilya\\\"},{\\\"family\\\":\\\"Hinton\\\",\\\"given\\\":\\\"Geoffrey E.\\\"}],\\\"container-title\\\":\\\"Commun. ACM\\\",\\\"id\\\":\\\"http://dl.acm.org/citation.cfm?doid=3098997.3065386\\\",\\\"issue\\\":\\\"6\\\",\\\"issued\\\":{\\\"date-parts\\\":[[2017,5]]},\\\"page\\\":\\\"84-90\\\",\\\"title\\\":\\\"ImageNet Classification with Deep Convolutional Neural Networks\\\",\\\"type\\\":\\\"article-journal\\\",\\\"volume\\\":\\\"60\\\"}\"]]],[{\"t\":\"Str\",\"c\":\"AlexNet\"}],[\"http://dl.acm.org/citation.cfm?doid=3098997.3065386\",\"\"]]},{\"t\":\"Superscript\",\"c\":[{\"t\":\"Cite\",\"c\":[[{\"citationSuffix\":[],\"citationNoteNum\":0,\"citationMode\":{\"t\":\"NormalCitation\"},\"citationPrefix\":[],\"citationId\":\"http://dl.acm.org/citation.cfm?doid=3098997.3065386\",\"citationHash\":20}],[{\"t\":\"Str\",\"c\":\"[\"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"18\"}],[\"#ref-http://dl.acm.org/citation.cfm?doid=3098997.3065386\",\"\"]]},{\"t\":\"Str\",\"c\":\"]\"}]]}]},{\"t\":\"Str\",\"c\":\" first introduced CNNs to the ImageNet challenge. \"},{\"t\":\"Cite\",\"c\":[[{\"citationSuffix\":[],\"citationNoteNum\":0,\"citationMode\":{\"t\":\"NormalCitation\"},\"citationPrefix\":[],\"citationId\":\"https://arxiv.org/abs/1409.1556\",\"citationHash\":21},{\"citationSuffix\":[],\"citationNoteNum\":0,\"citationMode\":{\"t\":\"NormalCitation\"},\"citationPrefix\":[],\"citationId\":\"https://ieeexplore.ieee.org/document/7298594\",\"citationHash\":22},{\"citationSuffix\":[],\"citationNoteNum\":0,\"citationMode\":{\"t\":\"NormalCitation\"},\"citationPrefix\":[],\"citationId\":\"https://ieeexplore.ieee.org/document/7780459\",\"citationHash\":23}],[{\"t\":\"Str\",\"c\":\"[\"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"19\"}],[\"#ref-https://arxiv.org/abs/1409.1556\",\"\"]]},{\"t\":\"Str\",\"c\":\"]–[\"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"21\"}],[\"#ref-https://ieeexplore.ieee.org/document/7780459\",\"\"]]},{\"t\":\"Str\",\"c\":\"]\"}]]},{\"t\":\"Str\",\"c\":\" further improved on the results.\"}]},{\"t\":\"Header\",\"c\":[1,[\"references\",[],[]],[{\"t\":\"Str\",\"c\":\"References\"}]]},{\"t\":\"Para\",\"c\":[]},{\"t\":\"Div\",\"c\":[[\"refs\",[\"references\"],[]],[{\"t\":\"Div\",\"c\":[[\"ref-https://github.com/phiresky/pandoc-url2cite\",[],[]],[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"[1] phiresky, “Effortlessly and transparently add correctly styled citations to your markdown paper given only a URL: Phiresky/pandoc-url2cite.” Dec-2019 [Online]. Available: \"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"https://github.com/phiresky/pandoc-url2cite\"}],[\"https://github.com/phiresky/pandoc-url2cite\",\"\"]]},{\"t\":\"Str\",\"c\":\". [Accessed: 14-Dec-2019]\"}]}]]},{\"t\":\"Div\",\"c\":[[\"ref-https://citationstyles.org/\",[],[]],[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"[2] Y. Name, “Citation Style Language,” \"},{\"t\":\"Emph\",\"c\":[{\"t\":\"Str\",\"c\":\"Citation Style Language\"}]},{\"t\":\"Str\",\"c\":\". [Online]. Available: \"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"https://citationstyles.org/\"}],[\"https://citationstyles.org/\",\"\"]]},{\"t\":\"Str\",\"c\":\". [Accessed: 14-Dec-2019]\"}]}]]},{\"t\":\"Div\",\"c\":[[\"ref-https://www.mendeley.com/\",[],[]],[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"[3] “Mendeley - Reference Management Software & Researcher Network,” \"},{\"t\":\"Emph\",\"c\":[{\"t\":\"Str\",\"c\":\"www.mendeley.com\"}]},{\"t\":\"Str\",\"c\":\". [Online]. Available: \"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"https://www.mendeley.com/?interaction_required=true\"}],[\"https://www.mendeley.com/?interaction_required=true\",\"\"]]},{\"t\":\"Str\",\"c\":\". [Accessed: 14-Dec-2019]\"}]}]]},{\"t\":\"Div\",\"c\":[[\"ref-https://www.zotero.org/\",[],[]],[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"[4] “Zotero Your personal research assistant,” \"},{\"t\":\"Emph\",\"c\":[{\"t\":\"Str\",\"c\":\"www.zotero.org\"}]},{\"t\":\"Str\",\"c\":\". [Online]. Available: \"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"https://www.zotero.org/\"}],[\"https://www.zotero.org/\",\"\"]]},{\"t\":\"Str\",\"c\":\". [Accessed: 14-Dec-2019]\"}]}]]},{\"t\":\"Div\",\"c\":[[\"ref-https://phiresky.github.io/blog/2019/pandoc-url2cite/\",[],[]],[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"[5] “Automatic citation extraction from URLs (draft) - phiresky’s blog,” \"},{\"t\":\"Emph\",\"c\":[{\"t\":\"Str\",\"c\":\"phiresky.github.io\"}]},{\"t\":\"Str\",\"c\":\". [Online]. Available: \"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"https://phiresky.github.io/blog/2019/pandoc-url2cite/\"}],[\"https://phiresky.github.io/blog/2019/pandoc-url2cite/\",\"\"]]},{\"t\":\"Str\",\"c\":\". [Accessed: 14-Dec-2019]\"}]}]]},{\"t\":\"Div\",\"c\":[[\"ref-https://opensource.com/article/18/9/pandoc-research-paper\",[],[]],[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"[6] F.-R. Kiko, “How to use Pandoc to produce a research paper,” \"},{\"t\":\"Emph\",\"c\":[{\"t\":\"Str\",\"c\":\"Opensource.com\"}]},{\"t\":\"Str\",\"c\":\". [Online]. Available: \"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"https://opensource.com/article/18/9/pandoc-research-paper\"}],[\"https://opensource.com/article/18/9/pandoc-research-paper\",\"\"]]},{\"t\":\"Str\",\"c\":\". [Accessed: 14-Dec-2019]\"}]}]]},{\"t\":\"Div\",\"c\":[[\"ref-https://pandoc.org/MANUAL.html#citations\",[],[]],[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"[7] “Pandoc - Pandoc User’s Guide,” \"},{\"t\":\"Emph\",\"c\":[{\"t\":\"Str\",\"c\":\"pandoc.org\"}]},{\"t\":\"Str\",\"c\":\". [Online]. Available: \"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"https://pandoc.org/MANUAL.html#citations\"}],[\"https://pandoc.org/MANUAL.html#citations\",\"\"]]},{\"t\":\"Str\",\"c\":\". [Accessed: 14-Dec-2019]\"}]}]]},{\"t\":\"Div\",\"c\":[[\"ref-https://pandoc.org/MANUAL.html#extension-yaml_metadata_block\",[],[]],[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"[8] “Pandoc - Pandoc User’s Guide,” \"},{\"t\":\"Emph\",\"c\":[{\"t\":\"Str\",\"c\":\"pandoc.org\"}]},{\"t\":\"Str\",\"c\":\". [Online]. Available: \"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"https://pandoc.org/MANUAL.html#extension-yaml_metadata_block\"}],[\"https://pandoc.org/MANUAL.html#extension-yaml_metadata_block\",\"\"]]},{\"t\":\"Str\",\"c\":\". [Accessed: 14-Dec-2019]\"}]}]]},{\"t\":\"Div\",\"c\":[[\"ref-https://github.com/retorquere/zotero-better-bibtex\",[],[]],[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"[9] E. Heyns, “Make Zotero effective for us LaTeX holdouts. Contribute to retorquere/zotero-better-bibtex development by creating an account on GitHub.” Dec-2019 [Online]. Available: \"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"https://github.com/retorquere/zotero-better-bibtex\"}],[\"https://github.com/retorquere/zotero-better-bibtex\",\"\"]]},{\"t\":\"Str\",\"c\":\". [Accessed: 14-Dec-2019]\"}]}]]},{\"t\":\"Div\",\"c\":[[\"ref-https://www.zotero.org/support/dev/translators\",[],[]],[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"[10] “Dev:Translators [Zotero Documentation],” \"},{\"t\":\"Emph\",\"c\":[{\"t\":\"Str\",\"c\":\"www.zotero.org\"}]},{\"t\":\"Str\",\"c\":\". [Online]. Available: \"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"https://www.zotero.org/support/dev/translators\"}],[\"https://www.zotero.org/support/dev/translators\",\"\"]]},{\"t\":\"Str\",\"c\":\". [Accessed: 14-Dec-2019]\"}]}]]},{\"t\":\"Div\",\"c\":[[\"ref-https://github.com/zotero/translation-server\",[],[]],[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"[11] “A Node.Js-based server to run Zotero translators. Contribute to zotero/translation-server development by creating an account on GitHub.” zotero, Dec-2019 [Online]. Available: \"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"https://github.com/zotero/translation-server\"}],[\"https://github.com/zotero/translation-server\",\"\"]]},{\"t\":\"Str\",\"c\":\". [Accessed: 14-Dec-2019]\"}]}]]},{\"t\":\"Div\",\"c\":[[\"ref-https://www.mediawiki.org/wiki/Citoid/API\",[],[]],[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"[12] “Citoid/API - MediaWiki,” \"},{\"t\":\"Emph\",\"c\":[{\"t\":\"Str\",\"c\":\"www.mediawiki.org\"}]},{\"t\":\"Str\",\"c\":\". [Online]. Available: \"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"https://www.mediawiki.org/wiki/Citoid/API\"}],[\"https://www.mediawiki.org/wiki/Citoid/API\",\"\"]]},{\"t\":\"Str\",\"c\":\". [Accessed: 14-Dec-2019]\"}]}]]},{\"t\":\"Div\",\"c\":[[\"ref-https://github.com/zotero/translation-server/issues/70\",[],[]],[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"[13] “Try translating PDF URLs based on URL · Issue #70 · zotero/translation-server,” \"},{\"t\":\"Emph\",\"c\":[{\"t\":\"Str\",\"c\":\"GitHub\"}]},{\"t\":\"Str\",\"c\":\". [Online]. Available: \"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"https://github.com/zotero/translation-server/issues/70\"}],[\"https://github.com/zotero/translation-server/issues/70\",\"\"]]},{\"t\":\"Str\",\"c\":\". [Accessed: 14-Dec-2019]\"}]}]]},{\"t\":\"Div\",\"c\":[[\"ref-https://www.overleaf.com/learn/latex/Bibtex_bibliography_styles\",[],[]],[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"[14] “Bibtex bibliography styles,” \"},{\"t\":\"Emph\",\"c\":[{\"t\":\"Str\",\"c\":\"www.overleaf.com\"}]},{\"t\":\"Str\",\"c\":\". [Online]. Available: \"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"https://www.overleaf.com/learn/latex/Bibtex_bibliography_styles\"}],[\"https://www.overleaf.com/learn/latex/Bibtex_bibliography_styles\",\"\"]]},{\"t\":\"Str\",\"c\":\". [Accessed: 14-Dec-2019]\"}]}]]},{\"t\":\"Div\",\"c\":[[\"ref-https://ogp.me/\",[],[]],[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"[15] “Open Graph protocol,” \"},{\"t\":\"Emph\",\"c\":[{\"t\":\"Str\",\"c\":\"ogp.me\"}]},{\"t\":\"Str\",\"c\":\". [Online]. Available: \"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"http://ogp.me/\"}],[\"http://ogp.me/\",\"\"]]},{\"t\":\"Str\",\"c\":\". [Accessed: 14-Dec-2019]\"}]}]]},{\"t\":\"Div\",\"c\":[[\"ref-https://github.com/jgm/pandoc-citeproc/issues/308\",[],[]],[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"[16] “Url as citekey/referencekey · Issue #308 · jgm/pandoc-citeproc,” \"},{\"t\":\"Emph\",\"c\":[{\"t\":\"Str\",\"c\":\"GitHub\"}]},{\"t\":\"Str\",\"c\":\". [Online]. Available: \"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"https://github.com/jgm/pandoc-citeproc/issues/308\"}],[\"https://github.com/jgm/pandoc-citeproc/issues/308\",\"\"]]},{\"t\":\"Str\",\"c\":\". [Accessed: 14-Dec-2019]\"}]}]]},{\"t\":\"Div\",\"c\":[[\"ref-https://github.com/phiresky/pandoc-url2cite/blob/master/example/doi-isbn.md\",[],[]],[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"[17] phiresky, “Effortlessly and transparently add correctly styled citations to your markdown paper given only a URL: Phiresky/pandoc-url2cite.” Dec-2019 [Online]. Available: \"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"https://github.com/phiresky/pandoc-url2cite\"}],[\"https://github.com/phiresky/pandoc-url2cite\",\"\"]]},{\"t\":\"Str\",\"c\":\". [Accessed: 14-Dec-2019]\"}]}]]},{\"t\":\"Div\",\"c\":[[\"ref-http://dl.acm.org/citation.cfm?doid=3098997.3065386\",[],[]],[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"[18] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet Classification with Deep Convolutional Neural Networks,” \"},{\"t\":\"Emph\",\"c\":[{\"t\":\"Str\",\"c\":\"Commun. ACM\"}]},{\"t\":\"Str\",\"c\":\", vol. 60, no. 6, pp. 84–90, May 2017 [Online]. Available: \"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"http://doi.acm.org/10.1145/3065386\"}],[\"http://doi.acm.org/10.1145/3065386\",\"\"]]},{\"t\":\"Str\",\"c\":\". [Accessed: 14-Dec-2019]\"}]}]]},{\"t\":\"Div\",\"c\":[[\"ref-https://arxiv.org/abs/1409.1556\",[],[]],[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"[19] K. Simonyan and A. Zisserman, “Very Deep Convolutional Networks for Large-Scale Image Recognition,” \"},{\"t\":\"Emph\",\"c\":[{\"t\":\"Str\",\"c\":\"arXiv:1409.1556 [cs]\"}]},{\"t\":\"Str\",\"c\":\", Sep. 2014 [Online]. Available: \"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"http://arxiv.org/abs/1409.1556\"}],[\"http://arxiv.org/abs/1409.1556\",\"\"]]},{\"t\":\"Str\",\"c\":\". [Accessed: 14-Dec-2019]\"}]}]]},{\"t\":\"Div\",\"c\":[[\"ref-https://ieeexplore.ieee.org/document/7298594\",[],[]],[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"[20] C. Szegedy \"},{\"t\":\"Emph\",\"c\":[{\"t\":\"Str\",\"c\":\"et al.\"}]},{\"t\":\"Str\",\"c\":\", “Going deeper with convolutions,” in \"},{\"t\":\"Emph\",\"c\":[{\"t\":\"Str\",\"c\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\"}]},{\"t\":\"Str\",\"c\":\", 2015, pp. 1–9 [Online]. Available: \"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"https://ieeexplore.ieee.org/document/7298594\"}],[\"https://ieeexplore.ieee.org/document/7298594\",\"\"]]},{\"t\":\"Str\",\"c\":\". [Accessed: 14-Dec-2019]\"}]}]]},{\"t\":\"Div\",\"c\":[[\"ref-https://ieeexplore.ieee.org/document/7780459\",[],[]],[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"[21] K. He, X. Zhang, S. Ren, and J. Sun, “Deep Residual Learning for Image Recognition,” in \"},{\"t\":\"Emph\",\"c\":[{\"t\":\"Str\",\"c\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\"}]},{\"t\":\"Str\",\"c\":\", 2016, pp. 770–778 [Online]. Available: \"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"https://ieeexplore.ieee.org/document/7780459\"}],[\"https://ieeexplore.ieee.org/document/7780459\",\"\"]]},{\"t\":\"Str\",\"c\":\". [Accessed: 14-Dec-2019]\"}]}]]}]]}]}");

/***/ }),

/***/ "../posts-built/2019/rga--ripgrep-for-zip-targz-docx-odt-epub-jpg.md.json":
/*!********************************************************************************!*\
  !*** ../posts-built/2019/rga--ripgrep-for-zip-targz-docx-odt-epub-jpg.md.json ***!
  \********************************************************************************/
/*! exports provided: filename, frontmatter, preview, content_ast, default */
/***/ (function(module) {

module.exports = JSON.parse("{\"filename\":\"2019/rga--ripgrep-for-zip-targz-docx-odt-epub-jpg.md\",\"frontmatter\":{\"references\":[{\"note\":\"original-date: 2019-08-19T22:24:50Z\",\"URL\":\"https://github.com/phiresky/pandoc-url2cite\",\"title-short\":\"Effortlessly and transparently add correctly styled citations to your markdown paper given only a URL\",\"author\":[{\"literal\":\"phiresky\"}],\"id\":\"https://github.com/phiresky/pandoc-url2cite\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"issued\":{\"date-parts\":[[\"2019\",\"12\"]]},\"title\":\"Effortlessly and transparently add correctly styled citations to your markdown paper given only a URL: Phiresky/pandoc-url2cite\",\"type\":\"no-type\"},{\"URL\":\"https://citationstyles.org/\",\"container-title\":\"Citation Style Language\",\"author\":[{\"family\":\"Name\",\"given\":\"Your\"}],\"id\":\"https://citationstyles.org/\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"abstract\":\"CitationStyles.org, home of the Citation Style Language (CSL), a popular open XML-based language to describe the formatting of citations and bibliographies.\",\"title\":\"Citation Style Language\",\"type\":\"no-type\"},{\"URL\":\"https://www.mendeley.com/?interaction_required=true\",\"container-title\":\"www.mendeley.com\",\"id\":\"https://www.mendeley.com/\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"abstract\":\"Mendeley is a free reference manager and an academic social network. Manage your research, showcase your work, connect and collaborate with over five million researchers worldwide.\",\"title\":\"Mendeley - Reference Management Software & Researcher Network\",\"type\":\"no-type\"},{\"URL\":\"https://www.zotero.org/\",\"container-title\":\"www.zotero.org\",\"id\":\"https://www.zotero.org/\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"abstract\":\"Zotero is a free, easy-to-use tool to help you collect, organize, cite, and share research.\",\"title\":\"Zotero Your personal research assistant\",\"type\":\"no-type\"},{\"URL\":\"https://phiresky.github.io/blog/2019/pandoc-url2cite/\",\"container-title\":\"phiresky.github.io\",\"id\":\"https://phiresky.github.io/blog/2019/pandoc-url2cite/\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"abstract\":\"pandoc-url2cite[1] allows you to instantly and transparently cite most papers directly given only a single URL. You simply add a URL of a publication, and it will replace that with a real citation in whatever CSL[2] style you want. This means you can avoid dealing with Mendeley[3] or Zotero[4] and\",\"title\":\"Automatic citation extraction from URLs (draft) - phiresky’s blog\",\"type\":\"no-type\"},{\"URL\":\"https://opensource.com/article/18/9/pandoc-research-paper\",\"container-title\":\"Opensource.com\",\"author\":[{\"family\":\"Kiko\",\"given\":\"Fernandez-Reyes\"}],\"id\":\"https://opensource.com/article/18/9/pandoc-research-paper\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"abstract\":\"Learn how to manage section references, figures, tables, and more in Markdown.\",\"title\":\"How to use Pandoc to produce a research paper\",\"type\":\"no-type\"},{\"URL\":\"https://pandoc.org/MANUAL.html#citations\",\"container-title\":\"pandoc.org\",\"id\":\"https://pandoc.org/MANUAL.html#citations\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"title\":\"Pandoc - Pandoc User’s Guide\",\"type\":\"no-type\"},{\"URL\":\"https://pandoc.org/MANUAL.html#extension-yaml_metadata_block\",\"container-title\":\"pandoc.org\",\"id\":\"https://pandoc.org/MANUAL.html#extension-yaml_metadata_block\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"title\":\"Pandoc - Pandoc User’s Guide\",\"type\":\"no-type\"},{\"note\":\"original-date: 2013-09-20T08:26:14Z\",\"URL\":\"https://github.com/retorquere/zotero-better-bibtex\",\"author\":[{\"family\":\"Heyns\",\"given\":\"Emiliano\"}],\"id\":\"https://github.com/retorquere/zotero-better-bibtex\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"issued\":{\"date-parts\":[[\"2019\",\"12\"]]},\"title\":\"Make Zotero effective for us LaTeX holdouts. Contribute to retorquere/zotero-better-bibtex development by creating an account on GitHub\",\"type\":\"no-type\"},{\"URL\":\"https://www.zotero.org/support/dev/translators\",\"title-short\":\"Dev\",\"container-title\":\"www.zotero.org\",\"id\":\"https://www.zotero.org/support/dev/translators\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"abstract\":\"Zotero is a free, easy-to-use tool to help you collect, organize, cite, and share research.\",\"title\":\"Dev:Translators [Zotero Documentation]\",\"type\":\"no-type\"},{\"note\":\"original-date: 2018-06-11T11:28:53Z\",\"URL\":\"https://github.com/zotero/translation-server\",\"id\":\"https://github.com/zotero/translation-server\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"issued\":{\"date-parts\":[[\"2019\",\"12\"]]},\"title\":\"A Node.Js-based server to run Zotero translators. Contribute to zotero/translation-server development by creating an account on GitHub\",\"type\":\"no-type\",\"publisher\":\"zotero\"},{\"URL\":\"https://www.mediawiki.org/wiki/Citoid/API\",\"container-title\":\"www.mediawiki.org\",\"id\":\"https://www.mediawiki.org/wiki/Citoid/API\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"title\":\"Citoid/API - MediaWiki\",\"type\":\"no-type\"},{\"URL\":\"https://github.com/zotero/translation-server/issues/70\",\"container-title\":\"GitHub\",\"id\":\"https://github.com/zotero/translation-server/issues/70\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"abstract\":\"Related to #38, but a few translators are able to function based on the URL, even when it&#39;s a PDF page. We should try to support those cases, before either trying PDF recognition (from #38) or ...\",\"title\":\"Try translating PDF URLs based on URL · Issue #70 · zotero/translation-server\",\"type\":\"no-type\"},{\"URL\":\"https://www.overleaf.com/learn/latex/Bibtex_bibliography_styles\",\"container-title\":\"www.overleaf.com\",\"id\":\"https://www.overleaf.com/learn/latex/Bibtex_bibliography_styles\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"abstract\":\"An online LaTeX editor that’s easy to use. No installation, real-time collaboration, version control, hundreds of LaTeX templates, and more.\",\"title\":\"Bibtex bibliography styles\",\"type\":\"no-type\"},{\"URL\":\"http://ogp.me/\",\"container-title\":\"ogp.me\",\"id\":\"https://ogp.me/\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"abstract\":\"The Open Graph protocol enables any web page to become a rich object in a social graph.\",\"title\":\"Open Graph protocol\",\"type\":\"no-type\"},{\"URL\":\"https://github.com/jgm/pandoc-citeproc/issues/308\",\"container-title\":\"GitHub\",\"id\":\"https://github.com/jgm/pandoc-citeproc/issues/308\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"abstract\":\"I want to use this citekey: [@https://ww3.cad.de/cgi-bin/ubb/forumdisplay.cgi?action=topics&amp;number=527] from this bib file: @misc{}https://ww3.cad.de/cgi-bin/ubb/forumdisplay.cgi?action=topics&a...\",\"title\":\"Url as citekey/referencekey · Issue #308 · jgm/pandoc-citeproc\",\"type\":\"no-type\"},{\"ISSN\":\"0001-0782\",\"DOI\":\"10.1145/3065386\",\"volume\":\"60\",\"URL\":\"http://doi.acm.org/10.1145/3065386\",\"page\":\"84-90\",\"container-title\":\"Commun. ACM\",\"author\":[{\"family\":\"Krizhevsky\",\"given\":\"Alex\"},{\"family\":\"Sutskever\",\"given\":\"Ilya\"},{\"family\":\"Hinton\",\"given\":\"Geoffrey E.\"}],\"id\":\"http://dl.acm.org/citation.cfm?doid=3098997.3065386\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"issued\":{\"date-parts\":[[\"2017\",\"5\"]]},\"abstract\":\"We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called \\\"dropout\\\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.\",\"title\":\"ImageNet Classification with Deep Convolutional Neural Networks\",\"type\":\"article-journal\",\"issue\":\"6\"},{\"note\":\"arXiv: 1409.1556\",\"URL\":\"http://arxiv.org/abs/1409.1556\",\"container-title\":\"arXiv:1409.1556 [cs]\",\"author\":[{\"family\":\"Simonyan\",\"given\":\"Karen\"},{\"family\":\"Zisserman\",\"given\":\"Andrew\"}],\"id\":\"https://arxiv.org/abs/1409.1556\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"issued\":{\"date-parts\":[[\"2014\",\"9\"]]},\"abstract\":\"In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.\",\"title\":\"Very Deep Convolutional Networks for Large-Scale Image Recognition\",\"type\":\"article-journal\",\"keyword\":\"Computer Science - Computer Vision and Pattern Recognition\"},{\"DOI\":\"10.1109/CVPR.2015.7298594\",\"URL\":\"https://ieeexplore.ieee.org/document/7298594\",\"page\":\"1-9\",\"container-title\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"author\":[{\"family\":\"Szegedy\",\"given\":\"C.\"},{\"literal\":\"Wei Liu\"},{\"literal\":\"Yangqing Jia\"},{\"family\":\"Sermanet\",\"given\":\"P.\"},{\"family\":\"Reed\",\"given\":\"S.\"},{\"family\":\"Anguelov\",\"given\":\"D.\"},{\"family\":\"Erhan\",\"given\":\"D.\"},{\"family\":\"Vanhoucke\",\"given\":\"V.\"},{\"family\":\"Rabinovich\",\"given\":\"A.\"}],\"id\":\"https://ieeexplore.ieee.org/document/7298594\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"issued\":{\"date-parts\":[[\"2015\",\"6\"]]},\"abstract\":\"We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.\",\"title\":\"Going deeper with convolutions\",\"type\":\"paper-conference\",\"keyword\":\"convolution, decision making, feature extraction, Hebbian learning, image classification, neural net architecture, resource allocation, convolutional neural network architecture, resource utilization, architectural decision, Hebbian principle, object classification, object detection, Computer architecture, Convolutional codes, Sparse matrices, Neural networks, Visualization, Object detection, Computer vision\"},{\"DOI\":\"10.1109/CVPR.2016.90\",\"URL\":\"https://ieeexplore.ieee.org/document/7780459\",\"page\":\"770-778\",\"container-title\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"author\":[{\"family\":\"He\",\"given\":\"K.\"},{\"family\":\"Zhang\",\"given\":\"X.\"},{\"family\":\"Ren\",\"given\":\"S.\"},{\"family\":\"Sun\",\"given\":\"J.\"}],\"id\":\"https://ieeexplore.ieee.org/document/7780459\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"issued\":{\"date-parts\":[[\"2016\",\"6\"]]},\"abstract\":\"Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.\",\"title\":\"Deep Residual Learning for Image Recognition\",\"type\":\"paper-conference\",\"keyword\":\"image classification, learning (artificial intelligence), neural nets, object detection, COCO segmentation, ImageNet localization, ILSVRC & COCO 2015 competitions, deep residual nets, COCO object detection dataset, visual recognition tasks, CIFAR-10, ILSVRC 2015 classification task, ImageNet test set, VGG nets, residual nets, ImageNet dataset, residual function learning, deeper neural network training, image recognition, deep residual learning, Training, Degradation, Complexity theory, Image recognition, Neural networks, Visualization, Image segmentation\"},{\"note\":\"original-date: 2019-08-19T22:24:50Z\",\"URL\":\"https://github.com/phiresky/pandoc-url2cite\",\"title-short\":\"Effortlessly and transparently add correctly styled citations to your markdown paper given only a URL\",\"author\":[{\"literal\":\"phiresky\"}],\"id\":\"https://github.com/phiresky/pandoc-url2cite/blob/master/example/doi-isbn.md\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"issued\":{\"date-parts\":[[\"2019\",\"12\"]]},\"title\":\"Effortlessly and transparently add correctly styled citations to your markdown paper given only a URL: Phiresky/pandoc-url2cite\",\"type\":\"no-type\"}],\"date\":\"2019-06-16\",\"csl\":\"ieee-with-url.csl\",\"url2cite-link-output\":\"sup\",\"updated\":\"2019-10-21\",\"title\":\"rga: ripgrep, but also search in PDFs, E-Books, Office documents, zip, tar.gz, etc.\"},\"preview\":\"rga is a line-oriented search tool that allows you to look for a regex in a multitude of file types. rga wraps the awesome ripgrep and enables it to search in pdf, docx, sqlite, jpg, zip, tar.*, movie subtitles (mkv, mp4), etc. github repo Linux build status Crates.io fearless\",\"content_ast\":[{\"t\":\"Para\",\"c\":[{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"rga\"}],[\"https://github.com/phiresky/ripgrep-all\",\"\"]]},{\"t\":\"Str\",\"c\":\" is a line-oriented search tool that allows you to look for a regex in a multitude of file types. rga wraps the awesome \"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"ripgrep\"}],[\"https://github.com/BurntSushi/ripgrep\",\"\"]]},{\"t\":\"Str\",\"c\":\" and enables it to search in pdf, docx, sqlite, jpg, zip, tar.*, movie subtitles (mkv, mp4), etc.\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Image\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"github repo\"}],[\"https://img.shields.io/badge/repo-github.com%2Fphiresky%2Fripgrep--all-informational.svg\",\"\"]]}],[\"https://github.com/phiresky/ripgrep-all\",\"\"]]},{\"t\":\"SoftBreak\"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Image\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"Linux build status\"}],[\"https://api.travis-ci.org/phiresky/ripgrep-all.svg\",\"\"]]}],[\"https://travis-ci.org/phiresky/ripgrep-all\",\"\"]]},{\"t\":\"SoftBreak\"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Image\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"Crates.io\"}],[\"https://img.shields.io/crates/v/ripgrep-all.svg\",\"\"]]}],[\"https://crates.io/crates/ripgrep-all\",\"\"]]},{\"t\":\"SoftBreak\"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Image\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"fearless concurrency\"}],[\"https://img.shields.io/badge/concurrency-fearless-success.svg\",\"\"]]}],[\"https://www.reddit.com/r/rustjerk/top/?sort=top&t=all\",\"\"]]}]},{\"t\":\"Header\",\"c\":[2,[\"examples\",[],[]],[{\"t\":\"Str\",\"c\":\"Examples\"}]]},{\"t\":\"Header\",\"c\":[3,[\"pdfs\",[],[]],[{\"t\":\"Str\",\"c\":\"PDFs\"}]]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Say you have a large folder of papers or lecture slides, and you can’t remember which one of them mentioned \"},{\"t\":\"Code\",\"c\":[[\"\",[],[]],\"GRU\"]},{\"t\":\"Str\",\"c\":\"s. With rga, you can just run this:\"}]},{\"t\":\"RawBlock\",\"c\":[\"html\",\"<pre class=\\\"ansi2html language-none\\\">~$ rga \\\"GRU\\\" slides/\\n<span class=\\\"ansi35\\\">slides/2016/winter1516_lecture14.pdf</span>\\nPage 34:   <span class=\\\"ansi1\\\"></span><span class=\\\"ansi1 ansi31\\\">GRU</span>                            LSTM\\nPage 35:   <span class=\\\"ansi1\\\"></span><span class=\\\"ansi1 ansi31\\\">GRU</span>                            CONV\\nPage 38:     - Try out <span class=\\\"ansi1\\\"></span><span class=\\\"ansi1 ansi31\\\">GRU</span>-RCN! (imo best model)\\n\\n<span class=\\\"ansi35\\\">slides/2018/cs231n_2018_ds08.pdf</span>\\nPage  3: ●   CNNs, GANs, RNNs, LSTMs, <span class=\\\"ansi1\\\"></span><span class=\\\"ansi1 ansi31\\\">GRU</span>\\nPage 35: ● 1) temporal pooling 2) RNN (e.g. LSTM, <span class=\\\"ansi1\\\"></span><span class=\\\"ansi1 ansi31\\\">GRU</span>)\\n\\n<span class=\\\"ansi35\\\">slides/2019/cs231n_2019_lecture10.pdf</span>\\nPage 103:   <span class=\\\"ansi1\\\"></span><span class=\\\"ansi1 ansi31\\\">GRU</span> [Learning phrase representations using rnn\\nPage 105:    - Common to use LSTM or <span class=\\\"ansi1\\\"></span><span class=\\\"ansi1 ansi31\\\">GRU</span>\\n\\n</pre>\"]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"and it will recursively find a string in pdfs, including if some of them are zipped up.\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"You can do mostly the same thing with \"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Code\",\"c\":[[\"\",[],[]],\"pdfgrep -r\"]}],[\"https://pdfgrep.org/\",\"\"]]},{\"t\":\"Str\",\"c\":\", but you will miss content in other file types and it will be much slower:\"}]},{\"t\":\"CodeBlock\",\"c\":[[\"\",[\"barchart\"],[]],\"title: Searching in 65 pdfs with 93 slides each\\nseries: run time (seconds, lower is better)\\ndata:\\n   pdfgrep: 19.16\\n   rga (first run): 2.95\\n   rga (subsequent runs): 0.092\"]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"On the first run rga is mostly faster because of multithreading, but on subsequent runs (with the same files but any regex query) rga will cache the text extraction, so it becomes almost as fast as searching in plain text files. All runs were done with a warm FS cache.\"}]},{\"t\":\"Header\",\"c\":[3,[\"other-files\",[],[]],[{\"t\":\"Str\",\"c\":\"Other files\"}]]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"rga will recursively descend into archives and match text in every file type it knows.\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Here is an example directory with different file types:\"}]},{\"t\":\"CodeBlock\",\"c\":[[\"\",[],[]],\"demo\\n├── greeting.mkv\\n├── hello.odt\\n├── hello.sqlite3\\n└── somearchive.zip\\n    ├── dir\\n    │   ├── greeting.docx\\n    │   └── inner.tar.gz\\n    │       └── greeting.pdf\\n    └── greeting.epub\"]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"(see the actual directory \"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"here\"}],[\"https://github.com/phiresky/ripgrep-all/tree/master/exampledir/demo\",\"\"]]},{\"t\":\"Str\",\"c\":\")\"}]},{\"t\":\"RawBlock\",\"c\":[\"html\",\"<pre class=\\\"ansi2html language-none\\\">~$ rga \\\"hello\\\" demo/\\n\\n<span class=\\\"ansi35\\\">demo/greeting.mkv</span>\\nmetadata: chapters.chapter.0.tags.title=\\\"Chapter 1: <span class=\\\"ansi1\\\"></span><span class=\\\"ansi1 ansi31\\\">Hello</span>\\\"\\n00:08.398 --&gt; 00:11.758: <span class=\\\"ansi1\\\"></span><span class=\\\"ansi1 ansi31\\\">Hello</span> from a movie!\\n\\n<span class=\\\"ansi35\\\">demo/hello.odt</span>\\n<span class=\\\"ansi1\\\"></span><span class=\\\"ansi1 ansi31\\\">Hello</span> from an OpenDocument file!\\n\\n<span class=\\\"ansi35\\\">demo/hello.sqlite3</span>\\ntbl: greeting='<span class=\\\"ansi1\\\"></span><span class=\\\"ansi1 ansi31\\\">hello</span>', from='sqlite database!'\\n\\n<span class=\\\"ansi35\\\">demo/somearchive.zip</span>\\ndir/greeting.docx: <span class=\\\"ansi1\\\"></span><span class=\\\"ansi1 ansi31\\\">Hello</span> from a MS Office document!\\ndir/inner.tar.gz: greeting.pdf: Page 1: <span class=\\\"ansi1\\\"></span><span class=\\\"ansi1 ansi31\\\">Hello</span> from a PDF!\\ngreeting.epub: <span class=\\\"ansi1\\\"></span><span class=\\\"ansi1 ansi31\\\">Hello</span> from an E-Book!\\n</pre>\"]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"It can even search jpg / png images and scanned pdfs using OCR, though this is disabled by default since it is not useful that often and pretty slow.\"}]},{\"t\":\"RawBlock\",\"c\":[\"html\",\"<pre class=\\\"ansi2html language-none\\\">~$ # find screenshot of crates.io\\n~$ rga crates ~/screenshots --rga-adapters=+pdfpages,tesseract\\n<span class=\\\"ansi35\\\">screenshots/2019-06-14-19-01-10.png</span>\\n<span class=\\\"ansi1\\\"></span><span class=\\\"ansi1 ansi31\\\">crates</span>.io I Browse All <span class=\\\"ansi1\\\"></span><span class=\\\"ansi1 ansi31\\\">Crates</span>  Docs v\\nDocumentation Repository Dependent <span class=\\\"ansi1\\\"></span><span class=\\\"ansi1 ansi31\\\">crates</span>\\n\\n~$ # there it is!\\n</pre>\"]},{\"t\":\"Header\",\"c\":[2,[\"setup\",[],[]],[{\"t\":\"Str\",\"c\":\"Setup\"}]]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Linux, Windows and OSX binaries are available in GitHub releases. See \"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"the readme\"}],[\"https://github.com/phiresky/ripgrep-all#installation\",\"\"]]},{\"t\":\"Str\",\"c\":\" for more information.\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"For Arch Linux, I have packaged \"},{\"t\":\"Code\",\"c\":[[\"\",[],[]],\"rga\"]},{\"t\":\"Str\",\"c\":\" in the AUR: \"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Code\",\"c\":[[\"\",[],[]],\"yay -S ripgrep-all\"]}],[\"https://aur.archlinux.org/packages/ripgrep-all/\",\"\"]]}]},{\"t\":\"Header\",\"c\":[2,[\"technical-details\",[],[]],[{\"t\":\"Str\",\"c\":\"Technical details\"}]]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"The code and a few more details are here: \"},{\"t\":\"Link\",\"c\":[[\"\",[\"uri\"],[]],[{\"t\":\"Str\",\"c\":\"https://github.com/phiresky/ripgrep-all\"}],[\"https://github.com/phiresky/ripgrep-all\",\"\"]]}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Code\",\"c\":[[\"\",[],[]],\"rga\"]},{\"t\":\"Str\",\"c\":\" simply runs ripgrep (\"},{\"t\":\"Code\",\"c\":[[\"\",[],[]],\"rg\"]},{\"t\":\"Str\",\"c\":\") with some options set, especially \"},{\"t\":\"Code\",\"c\":[[\"\",[],[]],\"--pre=rga-preproc\"]},{\"t\":\"Str\",\"c\":\" and \"},{\"t\":\"Code\",\"c\":[[\"\",[],[]],\"--pre-glob\"]},{\"t\":\"Str\",\"c\":\".\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Code\",\"c\":[[\"\",[],[]],\"rga-preproc [fname]\"]},{\"t\":\"Str\",\"c\":\" will match an \"},{\"t\":\"Quoted\",\"c\":[{\"t\":\"DoubleQuote\"},[{\"t\":\"Str\",\"c\":\"adapter\"}]]},{\"t\":\"Str\",\"c\":\" to the given file based on either it’s filename or it’s mime type (if \"},{\"t\":\"Code\",\"c\":[[\"\",[],[]],\"--rga-accurate\"]},{\"t\":\"Str\",\"c\":\" is given). You can see all adapters currently included in \"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"src/adapters\"}],[\"https://github.com/phiresky/ripgrep-all/tree/master/src/adapters\",\"\"]]},{\"t\":\"Str\",\"c\":\".\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Some rga adapters run external binaries to do the actual work (such as pandoc or ffmpeg), usually by writing to stdin and reading from stdout. Others use a Rust library or bindings to achieve the same effect (like sqlite or zip).\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"To read archives, the \"},{\"t\":\"Code\",\"c\":[[\"\",[],[]],\"zip\"]},{\"t\":\"Str\",\"c\":\" and \"},{\"t\":\"Code\",\"c\":[[\"\",[],[]],\"tar\"]},{\"t\":\"Str\",\"c\":\" libraries are used, which work fully in a streaming fashion - this means that the RAM usage is low and no data is ever actually extracted to disk!\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Most adapters read the files from a \"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"Read\"}],[\"https://doc.rust-lang.org/std/io/trait.Read.html\",\"\"]]},{\"t\":\"Str\",\"c\":\", so they work completely on streamed data (that can come from anywhere including within nested archives).\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"During the extraction, rga-preproc will compress the data with ZSTD to a memory cache while simultaneously writing it uncompressed to stdout. After completion, if the memory cache is smaller than 2MByte, it is written to a \"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"rkv\"}],[\"https://docs.rs/rkv/0.9.6/rkv/\",\"\"]]},{\"t\":\"Str\",\"c\":\" cache. The cache is keyed by (adapter, filename, mtime), so if a file changes it’s content is extracted again.\"}]},{\"t\":\"Header\",\"c\":[2,[\"future-work\",[],[]],[{\"t\":\"Str\",\"c\":\"Future Work\"}]]},{\"t\":\"BulletList\",\"c\":[[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"I wanted to add a photograph adapter (based on object classification / detection) for fun, so you can grep for \"},{\"t\":\"Quoted\",\"c\":[{\"t\":\"DoubleQuote\"},[{\"t\":\"Str\",\"c\":\"mountain\"}]]},{\"t\":\"Str\",\"c\":\" and it will show pictures of mountains, like in Google Photos. It worked with \"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"YOLO\"}],[\"https://pjreddie.com/darknet/yolo/\",\"\"]]},{\"t\":\"Str\",\"c\":\", but something more useful and state-of-the art \"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"like this\"}],[\"https://github.com/aimagelab/show-control-and-tell\",\"\"]]},{\"t\":\"Str\",\"c\":\" proved very hard to integrate.\"}]}],[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"7z adapter (couldn’t find a nice to use Rust library with streaming)\"}]}],[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"Allow per-adapter configuration options (probably via env (RGA_ADAPTERXYZ_CONF=json))\"}]}],[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"Maybe use a different disk kv-store as a cache instead of rkv, because I had some \"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"weird problems\"}],[\"https://github.com/phiresky/ripgrep-all/blob/05835c1c42bc3575023a81e5494c5530078730fc/src/preproc_cache.rs#L30\",\"\"]]},{\"t\":\"Str\",\"c\":\" with that. SQLite is great. All other Rust alternatives I could find don’t allow writing from multiple processes.\"}]}],[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"Tests!\"}]}],[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"There’s some more (mostly technical) todos in the code I don’t know how to fix. Help wanted.\"}]}],[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"Other \"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"open issues\"}],[\"https://github.com/phiresky/ripgrep-all/issues\",\"\"]]}]}]]},{\"t\":\"Header\",\"c\":[2,[\"similar-tools\",[],[]],[{\"t\":\"Str\",\"c\":\"Similar tools\"}]]},{\"t\":\"BulletList\",\"c\":[[{\"t\":\"Plain\",\"c\":[{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"pdfgrep\"}],[\"https://pdfgrep.org/\",\"\"]]}]}],[{\"t\":\"Plain\",\"c\":[{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"this gist\"}],[\"https://gist.github.com/phiresky/5025490526ba70663ab3b8af6c40a8db\",\"\"]]},{\"t\":\"Str\",\"c\":\" has my proof of concept version of a caching extractor to use ripgrep as a replacement for pdfgrep.\"}]}],[{\"t\":\"Plain\",\"c\":[{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"this gist\"}],[\"https://gist.github.com/ColonolBuendia/314826e37ec35c616d70506c38dc65aa\",\"\"]]},{\"t\":\"Str\",\"c\":\" is a more extensive preprocessing script by \"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"@ColonolBuendia\"}],[\"https://github.com/ColonolBuendia\",\"\"]]}]}],[{\"t\":\"Plain\",\"c\":[{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"lesspipe\"}],[\"https://github.com/wofr06/lesspipe\",\"\"]]},{\"t\":\"Str\",\"c\":\" is a tool to make \"},{\"t\":\"Code\",\"c\":[[\"\",[],[]],\"less\"]},{\"t\":\"Str\",\"c\":\" work with many different file types. Different usecase, but similar in what it does.\"}]}]]}]}");

/***/ }),

/***/ "../posts-built/2019/writer.md.json":
/*!******************************************!*\
  !*** ../posts-built/2019/writer.md.json ***!
  \******************************************/
/*! exports provided: filename, frontmatter, preview, content_ast, default */
/***/ (function(module) {

module.exports = JSON.parse("{\"filename\":\"2019/writer.md\",\"frontmatter\":{\"references\":[{\"note\":\"original-date: 2019-08-19T22:24:50Z\",\"URL\":\"https://github.com/phiresky/pandoc-url2cite\",\"title-short\":\"Effortlessly and transparently add correctly styled citations to your markdown paper given only a URL\",\"author\":[{\"literal\":\"phiresky\"}],\"id\":\"https://github.com/phiresky/pandoc-url2cite\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"issued\":{\"date-parts\":[[\"2019\",\"12\"]]},\"title\":\"Effortlessly and transparently add correctly styled citations to your markdown paper given only a URL: Phiresky/pandoc-url2cite\",\"type\":\"no-type\"},{\"URL\":\"https://citationstyles.org/\",\"container-title\":\"Citation Style Language\",\"author\":[{\"family\":\"Name\",\"given\":\"Your\"}],\"id\":\"https://citationstyles.org/\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"abstract\":\"CitationStyles.org, home of the Citation Style Language (CSL), a popular open XML-based language to describe the formatting of citations and bibliographies.\",\"title\":\"Citation Style Language\",\"type\":\"no-type\"},{\"URL\":\"https://www.mendeley.com/?interaction_required=true\",\"container-title\":\"www.mendeley.com\",\"id\":\"https://www.mendeley.com/\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"abstract\":\"Mendeley is a free reference manager and an academic social network. Manage your research, showcase your work, connect and collaborate with over five million researchers worldwide.\",\"title\":\"Mendeley - Reference Management Software & Researcher Network\",\"type\":\"no-type\"},{\"URL\":\"https://www.zotero.org/\",\"container-title\":\"www.zotero.org\",\"id\":\"https://www.zotero.org/\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"abstract\":\"Zotero is a free, easy-to-use tool to help you collect, organize, cite, and share research.\",\"title\":\"Zotero Your personal research assistant\",\"type\":\"no-type\"},{\"URL\":\"https://phiresky.github.io/blog/2019/pandoc-url2cite/\",\"container-title\":\"phiresky.github.io\",\"id\":\"https://phiresky.github.io/blog/2019/pandoc-url2cite/\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"abstract\":\"pandoc-url2cite[1] allows you to instantly and transparently cite most papers directly given only a single URL. You simply add a URL of a publication, and it will replace that with a real citation in whatever CSL[2] style you want. This means you can avoid dealing with Mendeley[3] or Zotero[4] and\",\"title\":\"Automatic citation extraction from URLs (draft) - phiresky’s blog\",\"type\":\"no-type\"},{\"URL\":\"https://opensource.com/article/18/9/pandoc-research-paper\",\"container-title\":\"Opensource.com\",\"author\":[{\"family\":\"Kiko\",\"given\":\"Fernandez-Reyes\"}],\"id\":\"https://opensource.com/article/18/9/pandoc-research-paper\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"abstract\":\"Learn how to manage section references, figures, tables, and more in Markdown.\",\"title\":\"How to use Pandoc to produce a research paper\",\"type\":\"no-type\"},{\"URL\":\"https://pandoc.org/MANUAL.html#citations\",\"container-title\":\"pandoc.org\",\"id\":\"https://pandoc.org/MANUAL.html#citations\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"title\":\"Pandoc - Pandoc User’s Guide\",\"type\":\"no-type\"},{\"URL\":\"https://pandoc.org/MANUAL.html#extension-yaml_metadata_block\",\"container-title\":\"pandoc.org\",\"id\":\"https://pandoc.org/MANUAL.html#extension-yaml_metadata_block\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"title\":\"Pandoc - Pandoc User’s Guide\",\"type\":\"no-type\"},{\"note\":\"original-date: 2013-09-20T08:26:14Z\",\"URL\":\"https://github.com/retorquere/zotero-better-bibtex\",\"author\":[{\"family\":\"Heyns\",\"given\":\"Emiliano\"}],\"id\":\"https://github.com/retorquere/zotero-better-bibtex\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"issued\":{\"date-parts\":[[\"2019\",\"12\"]]},\"title\":\"Make Zotero effective for us LaTeX holdouts. Contribute to retorquere/zotero-better-bibtex development by creating an account on GitHub\",\"type\":\"no-type\"},{\"URL\":\"https://www.zotero.org/support/dev/translators\",\"title-short\":\"Dev\",\"container-title\":\"www.zotero.org\",\"id\":\"https://www.zotero.org/support/dev/translators\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"abstract\":\"Zotero is a free, easy-to-use tool to help you collect, organize, cite, and share research.\",\"title\":\"Dev:Translators [Zotero Documentation]\",\"type\":\"no-type\"},{\"note\":\"original-date: 2018-06-11T11:28:53Z\",\"URL\":\"https://github.com/zotero/translation-server\",\"id\":\"https://github.com/zotero/translation-server\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"issued\":{\"date-parts\":[[\"2019\",\"12\"]]},\"title\":\"A Node.Js-based server to run Zotero translators. Contribute to zotero/translation-server development by creating an account on GitHub\",\"type\":\"no-type\",\"publisher\":\"zotero\"},{\"URL\":\"https://www.mediawiki.org/wiki/Citoid/API\",\"container-title\":\"www.mediawiki.org\",\"id\":\"https://www.mediawiki.org/wiki/Citoid/API\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"title\":\"Citoid/API - MediaWiki\",\"type\":\"no-type\"},{\"URL\":\"https://github.com/zotero/translation-server/issues/70\",\"container-title\":\"GitHub\",\"id\":\"https://github.com/zotero/translation-server/issues/70\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"abstract\":\"Related to #38, but a few translators are able to function based on the URL, even when it&#39;s a PDF page. We should try to support those cases, before either trying PDF recognition (from #38) or ...\",\"title\":\"Try translating PDF URLs based on URL · Issue #70 · zotero/translation-server\",\"type\":\"no-type\"},{\"URL\":\"https://www.overleaf.com/learn/latex/Bibtex_bibliography_styles\",\"container-title\":\"www.overleaf.com\",\"id\":\"https://www.overleaf.com/learn/latex/Bibtex_bibliography_styles\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"abstract\":\"An online LaTeX editor that’s easy to use. No installation, real-time collaboration, version control, hundreds of LaTeX templates, and more.\",\"title\":\"Bibtex bibliography styles\",\"type\":\"no-type\"},{\"URL\":\"http://ogp.me/\",\"container-title\":\"ogp.me\",\"id\":\"https://ogp.me/\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"abstract\":\"The Open Graph protocol enables any web page to become a rich object in a social graph.\",\"title\":\"Open Graph protocol\",\"type\":\"no-type\"},{\"URL\":\"https://github.com/jgm/pandoc-citeproc/issues/308\",\"container-title\":\"GitHub\",\"id\":\"https://github.com/jgm/pandoc-citeproc/issues/308\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"abstract\":\"I want to use this citekey: [@https://ww3.cad.de/cgi-bin/ubb/forumdisplay.cgi?action=topics&amp;number=527] from this bib file: @misc{}https://ww3.cad.de/cgi-bin/ubb/forumdisplay.cgi?action=topics&a...\",\"title\":\"Url as citekey/referencekey · Issue #308 · jgm/pandoc-citeproc\",\"type\":\"no-type\"},{\"ISSN\":\"0001-0782\",\"DOI\":\"10.1145/3065386\",\"volume\":\"60\",\"URL\":\"http://doi.acm.org/10.1145/3065386\",\"page\":\"84-90\",\"container-title\":\"Commun. ACM\",\"author\":[{\"family\":\"Krizhevsky\",\"given\":\"Alex\"},{\"family\":\"Sutskever\",\"given\":\"Ilya\"},{\"family\":\"Hinton\",\"given\":\"Geoffrey E.\"}],\"id\":\"http://dl.acm.org/citation.cfm?doid=3098997.3065386\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"issued\":{\"date-parts\":[[\"2017\",\"5\"]]},\"abstract\":\"We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called \\\"dropout\\\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.\",\"title\":\"ImageNet Classification with Deep Convolutional Neural Networks\",\"type\":\"article-journal\",\"issue\":\"6\"},{\"note\":\"arXiv: 1409.1556\",\"URL\":\"http://arxiv.org/abs/1409.1556\",\"container-title\":\"arXiv:1409.1556 [cs]\",\"author\":[{\"family\":\"Simonyan\",\"given\":\"Karen\"},{\"family\":\"Zisserman\",\"given\":\"Andrew\"}],\"id\":\"https://arxiv.org/abs/1409.1556\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"issued\":{\"date-parts\":[[\"2014\",\"9\"]]},\"abstract\":\"In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.\",\"title\":\"Very Deep Convolutional Networks for Large-Scale Image Recognition\",\"type\":\"article-journal\",\"keyword\":\"Computer Science - Computer Vision and Pattern Recognition\"},{\"DOI\":\"10.1109/CVPR.2015.7298594\",\"URL\":\"https://ieeexplore.ieee.org/document/7298594\",\"page\":\"1-9\",\"container-title\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"author\":[{\"family\":\"Szegedy\",\"given\":\"C.\"},{\"literal\":\"Wei Liu\"},{\"literal\":\"Yangqing Jia\"},{\"family\":\"Sermanet\",\"given\":\"P.\"},{\"family\":\"Reed\",\"given\":\"S.\"},{\"family\":\"Anguelov\",\"given\":\"D.\"},{\"family\":\"Erhan\",\"given\":\"D.\"},{\"family\":\"Vanhoucke\",\"given\":\"V.\"},{\"family\":\"Rabinovich\",\"given\":\"A.\"}],\"id\":\"https://ieeexplore.ieee.org/document/7298594\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"issued\":{\"date-parts\":[[\"2015\",\"6\"]]},\"abstract\":\"We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.\",\"title\":\"Going deeper with convolutions\",\"type\":\"paper-conference\",\"keyword\":\"convolution, decision making, feature extraction, Hebbian learning, image classification, neural net architecture, resource allocation, convolutional neural network architecture, resource utilization, architectural decision, Hebbian principle, object classification, object detection, Computer architecture, Convolutional codes, Sparse matrices, Neural networks, Visualization, Object detection, Computer vision\"},{\"DOI\":\"10.1109/CVPR.2016.90\",\"URL\":\"https://ieeexplore.ieee.org/document/7780459\",\"page\":\"770-778\",\"container-title\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"author\":[{\"family\":\"He\",\"given\":\"K.\"},{\"family\":\"Zhang\",\"given\":\"X.\"},{\"family\":\"Ren\",\"given\":\"S.\"},{\"family\":\"Sun\",\"given\":\"J.\"}],\"id\":\"https://ieeexplore.ieee.org/document/7780459\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"issued\":{\"date-parts\":[[\"2016\",\"6\"]]},\"abstract\":\"Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.\",\"title\":\"Deep Residual Learning for Image Recognition\",\"type\":\"paper-conference\",\"keyword\":\"image classification, learning (artificial intelligence), neural nets, object detection, COCO segmentation, ImageNet localization, ILSVRC & COCO 2015 competitions, deep residual nets, COCO object detection dataset, visual recognition tasks, CIFAR-10, ILSVRC 2015 classification task, ImageNet test set, VGG nets, residual nets, ImageNet dataset, residual function learning, deeper neural network training, image recognition, deep residual learning, Training, Degradation, Complexity theory, Image recognition, Neural networks, Visualization, Image segmentation\"},{\"note\":\"original-date: 2019-08-19T22:24:50Z\",\"URL\":\"https://github.com/phiresky/pandoc-url2cite\",\"title-short\":\"Effortlessly and transparently add correctly styled citations to your markdown paper given only a URL\",\"author\":[{\"literal\":\"phiresky\"}],\"id\":\"https://github.com/phiresky/pandoc-url2cite/blob/master/example/doi-isbn.md\",\"accessed\":{\"date-parts\":[[\"2019\",\"12\",\"14\"]]},\"issued\":{\"date-parts\":[[\"2019\",\"12\"]]},\"title\":\"Effortlessly and transparently add correctly styled citations to your markdown paper given only a URL: Phiresky/pandoc-url2cite\",\"type\":\"no-type\"}],\"date\":\"July 17, 2006\",\"csl\":\"ieee-with-url.csl\",\"hidden\":true,\"author\":[\"John MacFarlane\",\"Anonymous\"],\"url2cite-link-output\":\"sup\",\"title\":\"Pandoc Test Suite\"},\"preview\":\"This is a set of tests for pandoc. Most of them are adapted from John Gruber’s markdown test suite.HeadersLevel 2 with an embedded linkLevel 3 with emphasisLevel 4Level 5Level 1Level 2 with emphasisLevel 3 with no blank lineLevel 2 with no blank lineParagraphs Here’s a regular paragraph. In\",\"content_ast\":[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"This is a set of tests for pandoc. Most of them are adapted from John Gruber’s markdown test suite.\"}]},{\"t\":\"HorizontalRule\"},{\"t\":\"Header\",\"c\":[1,[\"headers\",[],[]],[{\"t\":\"Str\",\"c\":\"Headers\"}]]},{\"t\":\"Header\",\"c\":[2,[\"level-2-with-an-embedded-link\",[],[]],[{\"t\":\"Str\",\"c\":\"Level 2 with an \"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"embedded link\"}],[\"/url\",\"\"]]}]]},{\"t\":\"Header\",\"c\":[3,[\"level-3-with-emphasis\",[],[]],[{\"t\":\"Str\",\"c\":\"Level 3 with \"},{\"t\":\"Emph\",\"c\":[{\"t\":\"Str\",\"c\":\"emphasis\"}]}]]},{\"t\":\"Header\",\"c\":[4,[\"level-4\",[],[]],[{\"t\":\"Str\",\"c\":\"Level 4\"}]]},{\"t\":\"Header\",\"c\":[5,[\"level-5\",[],[]],[{\"t\":\"Str\",\"c\":\"Level 5\"}]]},{\"t\":\"Header\",\"c\":[1,[\"level-1\",[],[]],[{\"t\":\"Str\",\"c\":\"Level 1\"}]]},{\"t\":\"Header\",\"c\":[2,[\"level-2-with-emphasis\",[],[]],[{\"t\":\"Str\",\"c\":\"Level 2 with \"},{\"t\":\"Emph\",\"c\":[{\"t\":\"Str\",\"c\":\"emphasis\"}]}]]},{\"t\":\"Header\",\"c\":[3,[\"level-3\",[],[]],[{\"t\":\"Str\",\"c\":\"Level 3\"}]]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"with no blank line\"}]},{\"t\":\"Header\",\"c\":[2,[\"level-2\",[],[]],[{\"t\":\"Str\",\"c\":\"Level 2\"}]]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"with no blank line\"}]},{\"t\":\"HorizontalRule\"},{\"t\":\"Header\",\"c\":[1,[\"paragraphs\",[],[]],[{\"t\":\"Str\",\"c\":\"Paragraphs\"}]]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Here’s a regular paragraph.\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"In Markdown 1.0.0 and earlier. Version 8. This line turns into a list item. Because a hard-wrapped line in the middle of a paragraph looked like a list item.\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Here’s one with a bullet. * criminey.\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"There should be a hard line break\"},{\"t\":\"LineBreak\"},{\"t\":\"Str\",\"c\":\"here.\"}]},{\"t\":\"HorizontalRule\"},{\"t\":\"Header\",\"c\":[1,[\"block-quotes\",[],[]],[{\"t\":\"Str\",\"c\":\"Block Quotes\"}]]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"E-mail style:\"}]},{\"t\":\"BlockQuote\",\"c\":[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"This is a block quote. It is pretty short.\"}]}]},{\"t\":\"BlockQuote\",\"c\":[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Code in a block quote:\"}]},{\"t\":\"CodeBlock\",\"c\":[[\"\",[],[]],\"sub status {\\n    print \\\"working\\\";\\n}\"]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"A list:\"}]},{\"t\":\"OrderedList\",\"c\":[[1,{\"t\":\"Decimal\"},{\"t\":\"Period\"}],[[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"item one\"}]}],[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"item two\"}]}]]]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Nested block quotes:\"}]},{\"t\":\"BlockQuote\",\"c\":[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"nested\"}]}]},{\"t\":\"BlockQuote\",\"c\":[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"nested\"}]}]}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"This should not be a block quote: 2 > 1.\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"And a following paragraph.\"}]},{\"t\":\"HorizontalRule\"},{\"t\":\"Header\",\"c\":[1,[\"code-blocks\",[],[]],[{\"t\":\"Str\",\"c\":\"Code Blocks\"}]]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Code:\"}]},{\"t\":\"CodeBlock\",\"c\":[[\"\",[],[]],\"---- (should be four hyphens)\\n\\nsub status {\\n    print \\\"working\\\";\\n}\\n\\nthis code block is indented by one tab\"]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"And:\"}]},{\"t\":\"CodeBlock\",\"c\":[[\"\",[],[]],\"    this code block is indented by two tabs\\n\\nThese should not be escaped:  \\\\$ \\\\\\\\ \\\\> \\\\[ \\\\{\"]},{\"t\":\"HorizontalRule\"},{\"t\":\"Header\",\"c\":[1,[\"lists\",[],[]],[{\"t\":\"Str\",\"c\":\"Lists\"}]]},{\"t\":\"Header\",\"c\":[2,[\"unordered\",[],[]],[{\"t\":\"Str\",\"c\":\"Unordered\"}]]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Asterisks tight:\"}]},{\"t\":\"BulletList\",\"c\":[[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"asterisk 1\"}]}],[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"asterisk 2\"}]}],[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"asterisk 3\"}]}]]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Asterisks loose:\"}]},{\"t\":\"BulletList\",\"c\":[[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"asterisk 1\"}]}],[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"asterisk 2\"}]}],[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"asterisk 3\"}]}]]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Pluses tight:\"}]},{\"t\":\"BulletList\",\"c\":[[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"Plus 1\"}]}],[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"Plus 2\"}]}],[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"Plus 3\"}]}]]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Pluses loose:\"}]},{\"t\":\"BulletList\",\"c\":[[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Plus 1\"}]}],[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Plus 2\"}]}],[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Plus 3\"}]}]]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Minuses tight:\"}]},{\"t\":\"BulletList\",\"c\":[[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"Minus 1\"}]}],[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"Minus 2\"}]}],[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"Minus 3\"}]}]]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Minuses loose:\"}]},{\"t\":\"BulletList\",\"c\":[[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Minus 1\"}]}],[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Minus 2\"}]}],[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Minus 3\"}]}]]},{\"t\":\"Header\",\"c\":[2,[\"ordered\",[],[]],[{\"t\":\"Str\",\"c\":\"Ordered\"}]]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Tight:\"}]},{\"t\":\"OrderedList\",\"c\":[[1,{\"t\":\"Decimal\"},{\"t\":\"Period\"}],[[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"First\"}]}],[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"Second\"}]}],[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"Third\"}]}]]]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"and:\"}]},{\"t\":\"OrderedList\",\"c\":[[1,{\"t\":\"Decimal\"},{\"t\":\"Period\"}],[[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"One\"}]}],[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"Two\"}]}],[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"Three\"}]}]]]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Loose using tabs:\"}]},{\"t\":\"OrderedList\",\"c\":[[1,{\"t\":\"Decimal\"},{\"t\":\"Period\"}],[[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"First\"}]}],[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Second\"}]}],[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Third\"}]}]]]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"and using spaces:\"}]},{\"t\":\"OrderedList\",\"c\":[[1,{\"t\":\"Decimal\"},{\"t\":\"Period\"}],[[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"One\"}]}],[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Two\"}]}],[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Three\"}]}]]]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Multiple paragraphs:\"}]},{\"t\":\"OrderedList\",\"c\":[[1,{\"t\":\"Decimal\"},{\"t\":\"Period\"}],[[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Item 1, graf one.\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Item 1. graf two. The quick brown fox jumped over the lazy dog’s back.\"}]}],[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Item 2.\"}]}],[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Item 3.\"}]}]]]},{\"t\":\"Header\",\"c\":[2,[\"nested\",[],[]],[{\"t\":\"Str\",\"c\":\"Nested\"}]]},{\"t\":\"BulletList\",\"c\":[[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"Tab\"}]},{\"t\":\"BulletList\",\"c\":[[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"Tab\"}]},{\"t\":\"BulletList\",\"c\":[[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"Tab\"}]}]]}]]}]]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Here’s another:\"}]},{\"t\":\"OrderedList\",\"c\":[[1,{\"t\":\"Decimal\"},{\"t\":\"Period\"}],[[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"First\"}]}],[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"Second:\"}]},{\"t\":\"BulletList\",\"c\":[[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"Fee\"}]}],[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"Fie\"}]}],[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"Foe\"}]}]]}],[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"Third\"}]}]]]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Same thing but with paragraphs:\"}]},{\"t\":\"OrderedList\",\"c\":[[1,{\"t\":\"Decimal\"},{\"t\":\"Period\"}],[[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"First\"}]}],[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Second:\"}]},{\"t\":\"BulletList\",\"c\":[[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"Fee\"}]}],[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"Fie\"}]}],[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"Foe\"}]}]]}],[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Third\"}]}]]]},{\"t\":\"Header\",\"c\":[2,[\"tabs-and-spaces\",[],[]],[{\"t\":\"Str\",\"c\":\"Tabs and spaces\"}]]},{\"t\":\"BulletList\",\"c\":[[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"this is a list item indented with tabs\"}]}],[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"this is a list item indented with spaces\"}]},{\"t\":\"BulletList\",\"c\":[[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"this is an example list item indented with tabs\"}]}],[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"this is an example list item indented with spaces\"}]}]]}]]},{\"t\":\"Header\",\"c\":[2,[\"fancy-list-markers\",[],[]],[{\"t\":\"Str\",\"c\":\"Fancy list markers\"}]]},{\"t\":\"OrderedList\",\"c\":[[2,{\"t\":\"Decimal\"},{\"t\":\"TwoParens\"}],[[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"begins with 2\"}]}],[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"and now 3\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"with a continuation\"}]},{\"t\":\"OrderedList\",\"c\":[[4,{\"t\":\"LowerRoman\"},{\"t\":\"Period\"}],[[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"sublist with roman numerals, starting with 4\"}]}],[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"more items\"}]},{\"t\":\"OrderedList\",\"c\":[[1,{\"t\":\"UpperAlpha\"},{\"t\":\"TwoParens\"}],[[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"a subsublist\"}]}],[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"a subsublist\"}]}]]]}]]]}]]]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Nesting:\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"A. Upper Alpha I. Upper Roman. (6) Decimal start with 6 c) Lower alpha with paren\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Autonumbering:\"}]},{\"t\":\"OrderedList\",\"c\":[[1,{\"t\":\"Decimal\"},{\"t\":\"Period\"}],[[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"Autonumber.\"}]}],[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"More.\"}]},{\"t\":\"OrderedList\",\"c\":[[1,{\"t\":\"Decimal\"},{\"t\":\"Period\"}],[[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"Nested.\"}]}]]]}]]]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Should not be a list item:\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"M.A. 2007\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"B. Williams\"}]},{\"t\":\"HorizontalRule\"},{\"t\":\"Header\",\"c\":[1,[\"definition-lists\",[],[]],[{\"t\":\"Str\",\"c\":\"Definition Lists\"}]]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Tight using spaces:\"}]},{\"t\":\"DefinitionList\",\"c\":[[[{\"t\":\"Str\",\"c\":\"apple\"}],[[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"red fruit\"}]}]]],[[{\"t\":\"Str\",\"c\":\"orange\"}],[[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"orange fruit\"}]}]]],[[{\"t\":\"Str\",\"c\":\"banana\"}],[[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"yellow fruit\"}]}]]]]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Tight using tabs:\"}]},{\"t\":\"DefinitionList\",\"c\":[[[{\"t\":\"Str\",\"c\":\"apple\"}],[[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"red fruit\"}]}]]],[[{\"t\":\"Str\",\"c\":\"orange\"}],[[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"orange fruit\"}]}]]],[[{\"t\":\"Str\",\"c\":\"banana\"}],[[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"yellow fruit\"}]}]]]]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Loose:\"}]},{\"t\":\"DefinitionList\",\"c\":[[[{\"t\":\"Str\",\"c\":\"apple\"}],[[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"red fruit\"}]}]]],[[{\"t\":\"Str\",\"c\":\"orange\"}],[[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"orange fruit\"}]}]]],[[{\"t\":\"Str\",\"c\":\"banana\"}],[[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"yellow fruit\"}]}]]]]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Multiple blocks with italics:\"}]},{\"t\":\"DefinitionList\",\"c\":[[[{\"t\":\"Emph\",\"c\":[{\"t\":\"Str\",\"c\":\"apple\"}]}],[[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"red fruit\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"contains seeds, crisp, pleasant to taste\"}]}]]],[[{\"t\":\"Emph\",\"c\":[{\"t\":\"Str\",\"c\":\"orange\"}]}],[[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"orange fruit\"}]},{\"t\":\"CodeBlock\",\"c\":[[\"\",[],[]],\"{ orange code block }\"]},{\"t\":\"BlockQuote\",\"c\":[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"orange block quote\"}]}]}]]]]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Multiple definitions, tight:\"}]},{\"t\":\"DefinitionList\",\"c\":[[[{\"t\":\"Str\",\"c\":\"apple\"}],[[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"red fruit\"}]}],[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"computer\"}]}]]],[[{\"t\":\"Str\",\"c\":\"orange\"}],[[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"orange fruit\"}]}],[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"bank\"}]}]]]]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Multiple definitions, loose:\"}]},{\"t\":\"DefinitionList\",\"c\":[[[{\"t\":\"Str\",\"c\":\"apple\"}],[[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"red fruit\"}]}],[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"computer\"}]}]]],[[{\"t\":\"Str\",\"c\":\"orange\"}],[[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"orange fruit\"}]}],[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"bank\"}]}]]]]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Blank line after term, indented marker, alternate markers:\"}]},{\"t\":\"DefinitionList\",\"c\":[[[{\"t\":\"Str\",\"c\":\"apple\"}],[[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"red fruit\"}]}],[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"computer\"}]}]]],[[{\"t\":\"Str\",\"c\":\"orange\"}],[[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"orange fruit\"}]},{\"t\":\"OrderedList\",\"c\":[[1,{\"t\":\"Decimal\"},{\"t\":\"Period\"}],[[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"sublist\"}]}],[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"sublist\"}]}]]]}]]]]},{\"t\":\"Header\",\"c\":[1,[\"html-blocks\",[],[]],[{\"t\":\"Str\",\"c\":\"HTML Blocks\"}]]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Simple block on one line:\"}]},{\"t\":\"Div\",\"c\":[[\"\",[],[]],[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"foo\"}]}]]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"And nested without indentation:\"}]},{\"t\":\"Div\",\"c\":[[\"\",[],[]],[{\"t\":\"Div\",\"c\":[[\"\",[],[]],[{\"t\":\"Div\",\"c\":[[\"\",[],[]],[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"foo\"}]}]]}]]},{\"t\":\"Div\",\"c\":[[\"\",[],[]],[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"bar\"}]}]]}]]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Interpreted markdown in a table:\"}]},{\"t\":\"RawBlock\",\"c\":[\"html\",\"<table>\"]},{\"t\":\"RawBlock\",\"c\":[\"html\",\"<tr>\"]},{\"t\":\"RawBlock\",\"c\":[\"html\",\"<td>\"]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"This is \"},{\"t\":\"Emph\",\"c\":[{\"t\":\"Str\",\"c\":\"emphasized\"}]}]},{\"t\":\"RawBlock\",\"c\":[\"html\",\"</td>\"]},{\"t\":\"RawBlock\",\"c\":[\"html\",\"<td>\"]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"And this is \"},{\"t\":\"Strong\",\"c\":[{\"t\":\"Str\",\"c\":\"strong\"}]}]},{\"t\":\"RawBlock\",\"c\":[\"html\",\"</td>\"]},{\"t\":\"RawBlock\",\"c\":[\"html\",\"</tr>\"]},{\"t\":\"RawBlock\",\"c\":[\"html\",\"</table>\"]},{\"t\":\"RawBlock\",\"c\":[\"html\",\"<script type=\\\"text/javascript\\\">document.write('This *should not* be interpreted as markdown');</script>\"]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Here’s a simple block:\"}]},{\"t\":\"Div\",\"c\":[[\"\",[],[]],[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"foo\"}]}]]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"This should be a code block, though:\"}]},{\"t\":\"CodeBlock\",\"c\":[[\"\",[],[]],\"<div>\\n    foo\\n</div>\"]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"As should this:\"}]},{\"t\":\"CodeBlock\",\"c\":[[\"\",[],[]],\"<div>foo</div>\"]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Now, nested:\"}]},{\"t\":\"Div\",\"c\":[[\"\",[],[]],[{\"t\":\"Div\",\"c\":[[\"\",[],[]],[{\"t\":\"Div\",\"c\":[[\"\",[],[]],[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"foo\"}]}]]}]]}]]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"This should just be an HTML comment:\"}]},{\"t\":\"RawBlock\",\"c\":[\"html\",\"<!-- Comment -->\"]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Multiline:\"}]},{\"t\":\"RawBlock\",\"c\":[\"html\",\"<!--\\nBlah\\nBlah\\n-->\"]},{\"t\":\"RawBlock\",\"c\":[\"html\",\"<!--\\n    This is another comment.\\n-->\"]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Code block:\"}]},{\"t\":\"CodeBlock\",\"c\":[[\"\",[],[]],\"<!-- Comment -->\"]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Just plain comment, with trailing spaces on the line:\"}]},{\"t\":\"RawBlock\",\"c\":[\"html\",\"<!-- foo -->\"]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Code:\"}]},{\"t\":\"CodeBlock\",\"c\":[[\"\",[],[]],\"<hr />\"]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Hr’s:\"}]},{\"t\":\"RawBlock\",\"c\":[\"html\",\"<hr>\"]},{\"t\":\"RawBlock\",\"c\":[\"html\",\"<hr />\"]},{\"t\":\"RawBlock\",\"c\":[\"html\",\"<hr />\"]},{\"t\":\"RawBlock\",\"c\":[\"html\",\"<hr>\"]},{\"t\":\"RawBlock\",\"c\":[\"html\",\"<hr />\"]},{\"t\":\"RawBlock\",\"c\":[\"html\",\"<hr />\"]},{\"t\":\"RawBlock\",\"c\":[\"html\",\"<hr class=\\\"foo\\\" id=\\\"bar\\\" />\"]},{\"t\":\"RawBlock\",\"c\":[\"html\",\"<hr class=\\\"foo\\\" id=\\\"bar\\\" />\"]},{\"t\":\"RawBlock\",\"c\":[\"html\",\"<hr class=\\\"foo\\\" id=\\\"bar\\\">\"]},{\"t\":\"HorizontalRule\"},{\"t\":\"Header\",\"c\":[1,[\"inline-markup\",[],[]],[{\"t\":\"Str\",\"c\":\"Inline Markup\"}]]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"This is \"},{\"t\":\"Emph\",\"c\":[{\"t\":\"Str\",\"c\":\"emphasized\"}]},{\"t\":\"Str\",\"c\":\", and so \"},{\"t\":\"Emph\",\"c\":[{\"t\":\"Str\",\"c\":\"is this\"}]},{\"t\":\"Str\",\"c\":\".\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"This is \"},{\"t\":\"Strong\",\"c\":[{\"t\":\"Str\",\"c\":\"strong\"}]},{\"t\":\"Str\",\"c\":\", and so \"},{\"t\":\"Strong\",\"c\":[{\"t\":\"Str\",\"c\":\"is this\"}]},{\"t\":\"Str\",\"c\":\".\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"An \"},{\"t\":\"Emph\",\"c\":[{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"emphasized link\"}],[\"/url\",\"\"]]}]},{\"t\":\"Str\",\"c\":\".\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Strong\",\"c\":[{\"t\":\"Emph\",\"c\":[{\"t\":\"Str\",\"c\":\"This is strong and em.\"}]}]}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"So is \"},{\"t\":\"Strong\",\"c\":[{\"t\":\"Emph\",\"c\":[{\"t\":\"Str\",\"c\":\"this\"}]}]},{\"t\":\"Str\",\"c\":\" word.\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Strong\",\"c\":[{\"t\":\"Emph\",\"c\":[{\"t\":\"Str\",\"c\":\"This is strong and em.\"}]}]}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"So is \"},{\"t\":\"Strong\",\"c\":[{\"t\":\"Emph\",\"c\":[{\"t\":\"Str\",\"c\":\"this\"}]}]},{\"t\":\"Str\",\"c\":\" word.\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"This is code: \"},{\"t\":\"Code\",\"c\":[[\"\",[],[]],\">\"]},{\"t\":\"Str\",\"c\":\", \"},{\"t\":\"Code\",\"c\":[[\"\",[],[]],\"$\"]},{\"t\":\"Str\",\"c\":\", \"},{\"t\":\"Code\",\"c\":[[\"\",[],[]],\"\\\\\"]},{\"t\":\"Str\",\"c\":\", \"},{\"t\":\"Code\",\"c\":[[\"\",[],[]],\"\\\\$\"]},{\"t\":\"Str\",\"c\":\", \"},{\"t\":\"Code\",\"c\":[[\"\",[],[]],\"<html>\"]},{\"t\":\"Str\",\"c\":\".\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Strikeout\",\"c\":[{\"t\":\"Str\",\"c\":\"This is \"},{\"t\":\"Emph\",\"c\":[{\"t\":\"Str\",\"c\":\"strikeout\"}]},{\"t\":\"Str\",\"c\":\".\"}]}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Superscripts: a\"},{\"t\":\"Superscript\",\"c\":[{\"t\":\"Str\",\"c\":\"bc\"}]},{\"t\":\"Str\",\"c\":\"d a\"},{\"t\":\"Superscript\",\"c\":[{\"t\":\"Emph\",\"c\":[{\"t\":\"Str\",\"c\":\"hello\"}]}]},{\"t\":\"Str\",\"c\":\" a\"},{\"t\":\"Superscript\",\"c\":[{\"t\":\"Str\",\"c\":\"hello there\"}]},{\"t\":\"Str\",\"c\":\".\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Subscripts: H\"},{\"t\":\"Subscript\",\"c\":[{\"t\":\"Str\",\"c\":\"2\"}]},{\"t\":\"Str\",\"c\":\"O, H\"},{\"t\":\"Subscript\",\"c\":[{\"t\":\"Str\",\"c\":\"23\"}]},{\"t\":\"Str\",\"c\":\"O, H\"},{\"t\":\"Subscript\",\"c\":[{\"t\":\"Str\",\"c\":\"many of them\"}]},{\"t\":\"Str\",\"c\":\"O.\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"These should not be superscripts or subscripts, because of the unescaped spaces: a^b c^d, a~b c~d.\"}]},{\"t\":\"HorizontalRule\"},{\"t\":\"Header\",\"c\":[1,[\"smart-quotes-ellipses-dashes\",[],[]],[{\"t\":\"Str\",\"c\":\"Smart quotes, ellipses, dashes\"}]]},{\"t\":\"Para\",\"c\":[{\"t\":\"Quoted\",\"c\":[{\"t\":\"DoubleQuote\"},[{\"t\":\"Str\",\"c\":\"Hello,\"}]]},{\"t\":\"Str\",\"c\":\" said the spider. \"},{\"t\":\"Quoted\",\"c\":[{\"t\":\"DoubleQuote\"},[{\"t\":\"Quoted\",\"c\":[{\"t\":\"SingleQuote\"},[{\"t\":\"Str\",\"c\":\"Shelob\"}]]},{\"t\":\"Str\",\"c\":\" is my name.\"}]]}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Quoted\",\"c\":[{\"t\":\"SingleQuote\"},[{\"t\":\"Str\",\"c\":\"A\"}]]},{\"t\":\"Str\",\"c\":\", \"},{\"t\":\"Quoted\",\"c\":[{\"t\":\"SingleQuote\"},[{\"t\":\"Str\",\"c\":\"B\"}]]},{\"t\":\"Str\",\"c\":\", and \"},{\"t\":\"Quoted\",\"c\":[{\"t\":\"SingleQuote\"},[{\"t\":\"Str\",\"c\":\"C\"}]]},{\"t\":\"Str\",\"c\":\" are letters.\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Quoted\",\"c\":[{\"t\":\"SingleQuote\"},[{\"t\":\"Str\",\"c\":\"Oak,\"}]]},{\"t\":\"Space\"},{\"t\":\"Quoted\",\"c\":[{\"t\":\"SingleQuote\"},[{\"t\":\"Str\",\"c\":\"elm,\"}]]},{\"t\":\"Str\",\"c\":\" and \"},{\"t\":\"Quoted\",\"c\":[{\"t\":\"SingleQuote\"},[{\"t\":\"Str\",\"c\":\"beech\"}]]},{\"t\":\"Str\",\"c\":\" are names of trees. So is \"},{\"t\":\"Quoted\",\"c\":[{\"t\":\"SingleQuote\"},[{\"t\":\"Str\",\"c\":\"pine.\"}]]}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Quoted\",\"c\":[{\"t\":\"SingleQuote\"},[{\"t\":\"Str\",\"c\":\"He said, \"},{\"t\":\"Quoted\",\"c\":[{\"t\":\"DoubleQuote\"},[{\"t\":\"Str\",\"c\":\"I want to go.\"}]]}]]},{\"t\":\"Str\",\"c\":\" Were you alive in the 70’s?\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Here is some quoted \"},{\"t\":\"Quoted\",\"c\":[{\"t\":\"SingleQuote\"},[{\"t\":\"Code\",\"c\":[[\"\",[],[]],\"code\"]}]]},{\"t\":\"Str\",\"c\":\" and a \"},{\"t\":\"Quoted\",\"c\":[{\"t\":\"DoubleQuote\"},[{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"quoted link\"}],[\"http://example.com/?foo=1&bar=2\",\"\"]]}]]},{\"t\":\"Str\",\"c\":\".\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Some dashes: one—two — three—four — five.\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Dashes between numbers: 5–7, 255–66, 1987–1999.\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Ellipses…and…and….\"}]},{\"t\":\"HorizontalRule\"},{\"t\":\"Header\",\"c\":[1,[\"latex\",[],[]],[{\"t\":\"Str\",\"c\":\"LaTeX\"}]]},{\"t\":\"BulletList\",\"c\":[[{\"t\":\"Plain\",\"c\":[{\"t\":\"RawInline\",\"c\":[\"tex\",\"\\\\cite[22-23]{smith.1899}\"]}]}],[{\"t\":\"Plain\",\"c\":[{\"t\":\"Math\",\"c\":[{\"t\":\"InlineMath\"},\"2+2=4\"]}]}],[{\"t\":\"Plain\",\"c\":[{\"t\":\"Math\",\"c\":[{\"t\":\"InlineMath\"},\"x \\\\in y\"]}]}],[{\"t\":\"Plain\",\"c\":[{\"t\":\"Math\",\"c\":[{\"t\":\"InlineMath\"},\"\\\\alpha \\\\wedge \\\\omega\"]}]}],[{\"t\":\"Plain\",\"c\":[{\"t\":\"Math\",\"c\":[{\"t\":\"InlineMath\"},\"223\"]}]}],[{\"t\":\"Plain\",\"c\":[{\"t\":\"Math\",\"c\":[{\"t\":\"InlineMath\"},\"p\"]},{\"t\":\"Str\",\"c\":\"-Tree\"}]}],[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"Here’s some display math: \"},{\"t\":\"Math\",\"c\":[{\"t\":\"DisplayMath\"},\"\\\\frac{d}{dx}f(x)=\\\\lim_{h\\\\to 0}\\\\frac{f(x+h)-f(x)}{h}\"]}]}],[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"Here’s one that has a line break in it: \"},{\"t\":\"Math\",\"c\":[{\"t\":\"InlineMath\"},\"\\\\alpha + \\\\omega \\\\times x^2\"]},{\"t\":\"Str\",\"c\":\".\"}]}]]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"These shouldn’t be math:\"}]},{\"t\":\"BulletList\",\"c\":[[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"To get the famous equation, write \"},{\"t\":\"Code\",\"c\":[[\"\",[],[]],\"$e = mc^2$\"]},{\"t\":\"Str\",\"c\":\".\"}]}],[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"$22,000 is a \"},{\"t\":\"Emph\",\"c\":[{\"t\":\"Str\",\"c\":\"lot\"}]},{\"t\":\"Str\",\"c\":\" of money. So is $34,000. (It worked if \"},{\"t\":\"Quoted\",\"c\":[{\"t\":\"DoubleQuote\"},[{\"t\":\"Str\",\"c\":\"lot\"}]]},{\"t\":\"Str\",\"c\":\" is emphasized.)\"}]}],[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"Shoes ($20) and socks ($5).\"}]}],[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"Escaped \"},{\"t\":\"Code\",\"c\":[[\"\",[],[]],\"$\"]},{\"t\":\"Str\",\"c\":\": $73 \"},{\"t\":\"Emph\",\"c\":[{\"t\":\"Str\",\"c\":\"this should be emphasized\"}]},{\"t\":\"Str\",\"c\":\" 23$.\"}]}]]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Here’s a LaTeX table:\"}]},{\"t\":\"RawBlock\",\"c\":[\"tex\",\"\\\\begin{tabular}{|l|l|}\\\\hline\\nAnimal & Number \\\\\\\\ \\\\hline\\nDog    & 2      \\\\\\\\\\nCat    & 1      \\\\\\\\ \\\\hline\\n\\\\end{tabular}\"]},{\"t\":\"HorizontalRule\"},{\"t\":\"Header\",\"c\":[1,[\"special-characters\",[],[]],[{\"t\":\"Str\",\"c\":\"Special Characters\"}]]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Here is some unicode:\"}]},{\"t\":\"BulletList\",\"c\":[[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"I hat: Î\"}]}],[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"o umlaut: ö\"}]}],[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"section: §\"}]}],[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"set membership: ∈\"}]}],[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"copyright: ©\"}]}]]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"AT&T has an ampersand in their name.\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"AT&T is another way to write it.\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"This & that.\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"4 < 5.\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"6 > 5.\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Backslash: \\\\\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Backtick: `\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Asterisk: *\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Underscore: _\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Left brace: {\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Right brace: }\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Left bracket: [\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Right bracket: ]\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Left paren: (\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Right paren: )\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Greater-than: >\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Hash: #\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Period: .\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Bang: !\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Plus: +\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Minus: -\"}]},{\"t\":\"HorizontalRule\"},{\"t\":\"Header\",\"c\":[1,[\"links\",[],[]],[{\"t\":\"Str\",\"c\":\"Links\"}]]},{\"t\":\"Header\",\"c\":[2,[\"explicit\",[],[]],[{\"t\":\"Str\",\"c\":\"Explicit\"}]]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Just a \"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"URL\"}],[\"/url/\",\"\"]]},{\"t\":\"Str\",\"c\":\".\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"URL and title\"}],[\"/url/\",\"title\"]]},{\"t\":\"Str\",\"c\":\".\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"URL and title\"}],[\"/url/\",\"title preceded by two spaces\"]]},{\"t\":\"Str\",\"c\":\".\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"URL and title\"}],[\"/url/\",\"title preceded by a tab\"]]},{\"t\":\"Str\",\"c\":\".\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"URL and title\"}],[\"/url/\",\"title with \\\"quotes\\\" in it\"]]}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"URL and title\"}],[\"/url/\",\"title with single quotes\"]]}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"with_underscore\"}],[\"/url/with_underscore\",\"\"]]}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"Email link\"}],[\"mailto:nobody@nowhere.net\",\"\"]]}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"Empty\"}],[\"\",\"\"]]},{\"t\":\"Str\",\"c\":\".\"}]},{\"t\":\"Header\",\"c\":[2,[\"reference\",[],[]],[{\"t\":\"Str\",\"c\":\"Reference\"}]]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Foo \"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"bar\"}],[\"/url/\",\"\"]]},{\"t\":\"Str\",\"c\":\".\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"With \"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"embedded [brackets]\"}],[\"/url/\",\"\"]]},{\"t\":\"Str\",\"c\":\".\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"b\"}],[\"/url/\",\"\"]]},{\"t\":\"Str\",\"c\":\" by itself should be a link.\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Indented \"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"once\"}],[\"/url\",\"\"]]},{\"t\":\"Str\",\"c\":\".\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Indented \"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"twice\"}],[\"/url\",\"\"]]},{\"t\":\"Str\",\"c\":\".\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Indented \"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"thrice\"}],[\"/url\",\"\"]]},{\"t\":\"Str\",\"c\":\".\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"This should [not][] be a link.\"}]},{\"t\":\"CodeBlock\",\"c\":[[\"\",[],[]],\"[not]: /url\"]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Foo \"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"bar\"}],[\"/url/\",\"Title with \\\"quotes\\\" inside\"]]},{\"t\":\"Str\",\"c\":\".\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Foo \"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"biz\"}],[\"/url/\",\"Title with \\\"quote\\\" inside\"]]},{\"t\":\"Str\",\"c\":\".\"}]},{\"t\":\"Header\",\"c\":[2,[\"with-ampersands\",[],[]],[{\"t\":\"Str\",\"c\":\"With ampersands\"}]]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Here’s a \"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"link with an ampersand in the URL\"}],[\"http://example.com/?foo=1&bar=2\",\"\"]]},{\"t\":\"Str\",\"c\":\".\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Here’s a link with an amersand in the link text: \"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"AT&T\"}],[\"http://att.com/\",\"AT&T\"]]},{\"t\":\"Str\",\"c\":\".\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Here’s an \"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"inline link\"}],[\"/script?foo=1&bar=2\",\"\"]]},{\"t\":\"Str\",\"c\":\".\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Here’s an \"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"inline link in pointy braces\"}],[\"/script?foo=1&bar=2\",\"\"]]},{\"t\":\"Str\",\"c\":\".\"}]},{\"t\":\"Header\",\"c\":[2,[\"autolinks\",[],[]],[{\"t\":\"Str\",\"c\":\"Autolinks\"}]]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"With an ampersand: \"},{\"t\":\"Link\",\"c\":[[\"\",[\"uri\"],[]],[{\"t\":\"Str\",\"c\":\"http://example.com/?foo=1&bar=2\"}],[\"http://example.com/?foo=1&bar=2\",\"\"]]}]},{\"t\":\"BulletList\",\"c\":[[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"In a list?\"}]}],[{\"t\":\"Plain\",\"c\":[{\"t\":\"Link\",\"c\":[[\"\",[\"uri\"],[]],[{\"t\":\"Str\",\"c\":\"http://example.com/\"}],[\"http://example.com/\",\"\"]]}]}],[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"It should.\"}]}]]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"An e-mail address: \"},{\"t\":\"Link\",\"c\":[[\"\",[\"email\"],[]],[{\"t\":\"Str\",\"c\":\"nobody@nowhere.net\"}],[\"mailto:nobody@nowhere.net\",\"\"]]}]},{\"t\":\"BlockQuote\",\"c\":[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Blockquoted: \"},{\"t\":\"Link\",\"c\":[[\"\",[\"uri\"],[]],[{\"t\":\"Str\",\"c\":\"http://example.com/\"}],[\"http://example.com/\",\"\"]]}]}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Auto-links should not occur here: \"},{\"t\":\"Code\",\"c\":[[\"\",[],[]],\"<http://example.com/>\"]}]},{\"t\":\"CodeBlock\",\"c\":[[\"\",[],[]],\"or here: <http://example.com/>\"]},{\"t\":\"HorizontalRule\"},{\"t\":\"Header\",\"c\":[1,[\"images\",[],[]],[{\"t\":\"Str\",\"c\":\"Images\"}]]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"From \"},{\"t\":\"Quoted\",\"c\":[{\"t\":\"DoubleQuote\"},[{\"t\":\"Str\",\"c\":\"Voyage dans la Lune\"}]]},{\"t\":\"Str\",\"c\":\" by Georges Melies (1902):\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Image\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"lalune\"}],[\"lalune.jpg\",\"fig:Voyage dans la Lune\"]]}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Here is a movie \"},{\"t\":\"Image\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"movie\"}],[\"movie.jpg\",\"\"]]},{\"t\":\"Str\",\"c\":\" icon.\"}]},{\"t\":\"HorizontalRule\"},{\"t\":\"Header\",\"c\":[1,[\"footnotes\",[],[]],[{\"t\":\"Str\",\"c\":\"Footnotes\"}]]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Here is a footnote reference,\"},{\"t\":\"Note\",\"c\":[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Here is the footnote. It can go anywhere after the footnote reference. It need not be placed at the end of the document.\"}]}]},{\"t\":\"Str\",\"c\":\" and another.\"},{\"t\":\"Note\",\"c\":[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Here’s the long note. This one contains multiple blocks.\"}]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Subsequent blocks are indented to show that they belong to the footnote (as with list items).\"}]},{\"t\":\"CodeBlock\",\"c\":[[\"\",[],[]],\"  { <code> }\"]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"If you want, you can indent every line, but you can also be lazy and just indent the first line of each block.\"}]}]},{\"t\":\"Str\",\"c\":\" This should \"},{\"t\":\"Emph\",\"c\":[{\"t\":\"Str\",\"c\":\"not\"}]},{\"t\":\"Str\",\"c\":\" be a footnote reference, because it contains a space.[^my note] Here is an inline note.\"},{\"t\":\"Note\",\"c\":[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"This is \"},{\"t\":\"Emph\",\"c\":[{\"t\":\"Str\",\"c\":\"easier\"}]},{\"t\":\"Str\",\"c\":\" to type. Inline notes may contain \"},{\"t\":\"Link\",\"c\":[[\"\",[],[]],[{\"t\":\"Str\",\"c\":\"links\"}],[\"http://google.com\",\"\"]]},{\"t\":\"Str\",\"c\":\" and \"},{\"t\":\"Code\",\"c\":[[\"\",[],[]],\"]\"]},{\"t\":\"Str\",\"c\":\" verbatim characters, as well as [bracketed text].\"}]}]}]},{\"t\":\"BlockQuote\",\"c\":[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Notes can go in quotes.\"},{\"t\":\"Note\",\"c\":[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"In quote.\"}]}]}]}]},{\"t\":\"OrderedList\",\"c\":[[1,{\"t\":\"Decimal\"},{\"t\":\"Period\"}],[[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"And in list items.\"},{\"t\":\"Note\",\"c\":[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"In list.\"}]}]}]}]]]},{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"This paragraph should not be part of the note, as it is not indented.\"}]}]}");

/***/ })

})
//# sourceMappingURL=post.js.58da5d46742fe43e7217.hot-update.js.map